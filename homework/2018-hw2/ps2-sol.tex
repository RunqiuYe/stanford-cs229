% HMC Math dept HW template example
% v0.04 by Eric J. Malm, 10 Mar 2005
\documentclass[12pt,letterpaper,boxed]{hmcpset}

% set 1-inch margins in the document
\usepackage[margin=1in]{geometry}
\usepackage{alltt}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathdots}
\usepackage{mathtools}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{pdflscape}
\usepackage{pgfplots}
\usepackage{siunitx}
\usepackage{slashed}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usepackage[normalem]{ulem}
\usepackage[all]{xy}
\usepackage{imakeidx}
\usepackage{enumerate}
\usepackage{physics}

% include this if you want to import graphics files with /includegraphics
\usepackage{graphicx}

\renewcommand{\tt}{t^{(i)}}
\newcommand{\ww}{w^{(i)}}
\renewcommand{\ss}{\sigma^{(i)}}
\newcommand{\ind}[1]{\mathbb{I}\{#1\}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\thetamap}{\theta_{\mathrm{MAP}}}
\renewcommand{\diag}{\mathrm{diag}}


% info for header block in upper right hand corner
\name{Runqiu Ye}
\class{Stanford CS299}
\assignment{Problem Set \#2}
\duedate{07/07/2024}

\linespread{1.15}
\begin{document}

\problemlist{Problem Set \#2: Supervised Learning II}

\begin{problem}[Problem 1]
\textbf{Logistic Regression: Training stability}
\end{problem}

\begin{solution}
  \begin{enumerate}[(a)]
    \item The most notable difference in training the logistic regression model on datasets $A$ and $B$ is that the algorithm does not converge on dataset $B$.
    
    \item To investigate why the training procedure behaves unexpectedly on dataset $B$, but not on $A$, we print the value of $\theta$ after every $10000$ iterations. We notice that for data set $B$, although the normalized $\frac{\theta}{\norm{\theta}}$ almost stop changing after several tens of thousands of iterations, each componenet of the unnormalized $\theta$ keeps increasing. We also notice that dataset $A$ is not linearly separable while dataset $B$ is linearly separable.
    
    From the code, we notice that the algorithm calculates the gradient of loss function as 
    \[
    \nabla_{\theta} J(\theta) = -\frac{1}{m} \sum_{i=1}^m \frac{ \yy \xx}{1+ \exp(\yy \theta^T \xx)}.
    \]
    From this, we know that the algorithm uses gradient descent to minimize the loss function
    \[
    J(\theta) = - \frac{1}{m} \sum_{i=1}^m \log \frac{1}{1+ \exp(-\yy \theta^T \xx)}.
    \]

    Hence, for a dataset that is linearly separable, that is, $\yy \theta^T \xx > 0$ for all $i$, a $\theta$ with larger norm always leads to a smaller loss, preventing the algorithm from converging. However, on a dataset that is not linearly separable, there exists $i$ such that $\yy \theta^T \xx < 0$. By plotting $f(z) = \log (1+e^{-z})$ in Figure \ref{fz}, we notice that negative margin dominates when scaling $\theta$ to a larger norm. Hence, we cannot always increase $\theta$ to a larger norm while minimizing $J(\theta)$.

    \begin{figure}
      \centering
      \includegraphics[width=0.6\linewidth]{fz.png}
      \caption{Plot of $f(z) = \log (1+e^{-z})$ for $-5 \leq z \leq 5$.}
      \label{fz}
    \end{figure}
    
    \item Consider the following modifications
    \begin{enumerate}[i.]
      \item Using a different constant learning rate will not make the algorithm converge on dataset $B$, since scaling $\theta$ to larger norm still always decreases the loss.
      \item Decreasing the learning rate over time will make the algorithm converge for dataset $B$, since in this way the change of $\theta$ converge to $0$.
      \item Linear scaling the input features does not help, since it does not change the dataset's linera separability.
      \item Adding a regularization term $\norm{\theta}_2^2$ helps, since now scaling $\theta$ to larger norm penalize the algorithm.
      \item Adding zero-mean Gaussian noise to the training data or labels helps as long as it makes the dataset not linearly separable.
    \end{enumerate}
    
    \item Support vector machines, which uses hinge loss, are not vulnerable to datasets like $B$. In SVM, geometric margin is considered, instead of functional margin considered here. In other words, $\theta$ is normalized, so for linearly separable datasets like $B$, the algorithm will still converge.
  \end{enumerate}
\end{solution}

\begin{problem}[Problem 2]
  \textbf{Model Calibration}

  Try to understand the output $h_\theta (x)$ of the hypothesis function of a logistic regression model, in particular why we might treat the output as a probability.

  When probabilities outputted by a model match empirical observation, the model is \emph{well-calibrated}. For example, if a set of examples $\xx$ for which $h_\theta(\xx) \approx 0.7$, around $70\%$ of those examples should have positive labels. In a well-calibrated model, this property holds true at every probability value. 

  Suppose training set $\{\xx, \yy\}_{i=1}^m$ with $\xx \in \R^{n+1}$ and $\yy \in \{0,1\}$. Assume we have an intercept term $\xx_0 = 1$ for all $i$. Let $\theta$ be the maximum likelihood parameters learned after training logistic regression model. In order for model to be well-calibrated, given any range of probabilities $(a,b)$ such that $0 \leq a < b \leq 1$, and trianing examples $\xx$ where the model outputs $h_\theta (\xx)$ fall in the range $(a,b)$, the fraction of positives in that set of examples should be equal to the average of the model outputs for those examples. That is,
  \[
  \frac{\sum_{i \in I_{a,b}} P(\yy = 1 \mid \xx; \theta)}{\abs{\{i \in I_{a,b}\}}} = \frac{\sum_{i \in I_{a,b}} \ind{\yy = 1}}{\abs{ \{i \in I_{a,b}\} }},
  \]
  where $P(\yy = 1 \mid x; \theta) = h_{\theta} (x) = 1/(1+\exp(-\theta^T x))$, $I_{a,b} = \{i : h_\theta(\xx) \in (a,b) \}$.
\end{problem}

\begin{solution}
  \begin{enumerate}[(a)]
    \item For the described logistic regression model over the range $(a,b) = (0,1)$, we want to show the above equality holds. Recall the gradient of log-likelihood
    \[
    \pdv{\ell}{\theta_j} = \sum_{i=1}^m (\yy - h_{\theta}(\xx)) \xx_j.
    \]
    For a maximum likelihood estimation, $\pdv{\ell}{\theta} = 0$. Hence $\pdv{\ell}{\theta_0} = 0$. Since $\xx_0 = 1$, we have
    \[
    \sum_{i=1}^{m} \yy - h_\theta(\xx) = 0.
    \]
    The desired equality follows immediately since $i \in I_{0,1}$ for all $i$.

    \item A perfectly calibrated model — that is, the equality holds for any $(a,b) \subset [0,1]$ — does not imply that the model achieves perfect accuracy. Consider $(a,b) = (\frac{1}{2}, 1)$, the above equality implies 
    \[
    \frac{\sum_{i \in I_{a,b}} P(\yy = 1 \mid \xx; \theta)}{\abs{\{i \in I_{a,b}\}}} = \frac{\sum_{i \in I_{a,b}} \ind{\yy = 1}}{\abs{ \{i \in I_{a,b}\} }} < 1.
    \]
    This shows that the model does not have perfect accuracy.

    For the converse direction, a perfect accuracy does not imply perfectly calibrated. Consider again $(a,b) = (\frac{1}{2}, 1)$, then we have
    \[
    \frac{\sum_{i \in I_{a,b}} \ind{\yy = 1}}{\abs{ \{i \in I_{a,b}\} }} = 1 > \frac{\sum_{i \in I_{a,b}} P(\yy = 1 \mid \xx; \theta)}{\abs{\{i \in I_{a,b}\}}}.
    \]

    \item Discuss what effect of $L_2$ regularization in the logistic regression objective has on model calibration. For $L_2$ regularization in logistic regression, the gradient becomes
    \[
    \pdv{\ell}{\theta_j} = \sum_{i=1}^m (\yy - h_{\theta}(\xx)) \xx_j - 2C \theta_j = 0.
    \]
    Hence, the equality does not hold unless $\theta_0 = 0$. 
  \end{enumerate}
\end{solution}

\begin{remark}
  The interval $(0,1)$ is the only range for which logistic regression is guaranteed to be calibrated. When GLM assumptions hold, all ranges $(a,b) \subset [0,1]$ are well calibrated. In addition, when test set has same distribution and when model has not overfit or underfit, logistic regression are well-calibrated on test data as well. Thus logistic regression is popular when we are interested in level of uncertainty in the model output. 
\end{remark}

\begin{problem}[Problem 3]
  \textbf{Bayesian Interpretation of Regularization}

  In Bayesian statistics, almost every quantity is a random variable. Joint distribution of all the random variables are called \emph{model} (e.g. $p(x,y,\theta)$). Every unknown quantity can be estimated by conditioning the model on all observed quantities. Such conditional distribution $p(\theta \mid x,y)$ is called \emph{posterior distribution}. A consequence of this approach is that we are required to endow a \emph{prior distribution} $p(\theta)$.

  In purest Bayesian interpretation, we are required to keep the entire posterior distribution over the parameters all the way until prediction to come up with the \emph{posterior predictive distribution}, and the final prediction will be the EV of the posterior predictive distribution. However, this is computationally very expensive.

  The compromise is to estimate a point value of the parameters instead of the full distribution, which is the mode of the posterior distribution. Estimating the mode of posterior distribution is also called \emph{maximum a posteriori estimation} (MAP). That is,
  \[
  \theta_{\mathrm{MAP}} = \argmax_{\theta} p(\theta \mid x,y).
  \]
  Compare this to the \emph{maximum likelihood estimation} (MLE):
  \[
  \theta_{\mathrm{MLE}} = \argmax_{\theta} p(y \mid x, \theta).
  \]

  In this problem, explore connections between MAP estimation and common regularization techniques that are applied with MLE estimation. In particular, we will show how choice of prior distribution over $\theta$ is equivalent to different kinds of regularization.
\end{problem}

\begin{solution}
  \begin{enumerate}[(a)]
    \item Assume that $p(\theta) = p(\theta \mid x)$, we have
    \[
    p(\theta \mid x,y) = \frac{p(y \mid \theta, x)p(\theta \mid x)}{p(y \mid x)} = \frac{p(y \mid \theta, x) p(\theta)}{p(y \mid x)}.
    \]
    Since $p(y \mid x)$ does not depend on $\theta$, 
    \[
    \theta_{\mathrm{MAP}} = \argmax_\theta p(y \mid \theta, x) p(\theta).
    \]
    Note that the assumption $p(\theta) = p(\theta \mid x)$ will be valid for models such as linear regression where the input $x$ are not explicitly modeled by $\theta$. Note also that this means $x$ and $\theta$ are marginally independent, but not conditionally independent when $y$ is given.

    \item Now we show that MAP estimation with a zero-mean Gaussian priori over $\theta$, specifically $\theta \sim \mathcal{N}(0, \eta^2 I)$, is equivalent to applying $L_2$ regularization with MLE estimation. Specifically, we need to show
    \[
    \theta_{\mathrm{MAP}} = \argmin_{\theta} -\log p(y \mid x, \theta) + \lambda \norm{\theta}_2^2.
    \]

    Recall the definition of multivariate normal, we have
    \[
    \begin{aligned}  
      \theta_{\mathrm{MAP}} &= \argmax_\theta p(\theta \mid x,y) \\
      &= \argmax_\theta p(y \mid \theta,x) p(\theta) \\
      &= \argmax_\theta p(y \mid \theta,x) \exp(-\frac{1}{2\eta^2} \norm{\theta}_2^2),
    \end{aligned}
    \]
    where we have ignored some of the constants. Taking the negative log on both sides, it follows that
    \[
    \theta_{\mathrm{MAP}} = \argmin_\theta -\log p(y \mid x, \theta) + \frac{1}{2\eta^2} \norm{\theta}_2^2,
    \]
    as desired, where $\lambda = \frac{1}{2\eta^2}$.

    \item Now consider a specific instance, a linear regression model given by $y = \theta^T x + \epsilon$, where $\epsilon \sim \mathcal{N}(0, \sigma^2)$. Like before, assume a Gaussian prior on this model such that $\theta \sim \mathcal{N}(0,\eta^2 I).$ Let $X$ be the design matrix of all training examples where each row is one example input, and $y$ be the column vector of all the example outputs. We want to derive a closed form expression for $\theta_{\mathrm{MAP}}$.
    
    For this model, the likelihood of an example $(\xx, \yy)$ is
    \[
    p(\yy \mid \xx, \theta) = \frac{1}{\sqrt{2\pi} \sigma} \exp(-\frac{(\yy - \theta^T \xx)^2}{2\sigma^2}).
    \]
    Hence,
    \[
    \log p(\yy \mid \xx, \theta) = - \frac{(\yy - \theta^T \xx)^2}{2\sigma^2} + C,
    \]
    where $C$ is some constant, and $\theta_{\mathrm{MAP}}$ is given by
    \[
    \begin{aligned}
      \theta_{\mathrm{MAP}} &= \argmin_\theta \sum_{i=1}^m \frac{(\yy - \theta^T \xx)^2}{2\sigma^2} + \frac{1}{2\eta^2} \norm{\theta}_2^2 \\
      &= \argmin_\theta \frac{1}{2\sigma^2} (y - X\theta)^T (y - X\theta) + \frac{1}{2\eta^2} \theta^T \theta.
    \end{aligned}
    \]
    Set the gradient of the function to 0, we have
    \[
    0 = -\frac{1}{\sigma^2} X^T(y-X\thetamap) + \frac{1}{\eta^2} \thetamap
    \]
    It follows that
    \[
    \thetamap = \eta^2 (\eta^2 X^T X + \sigma^2 I)^{-1} X^T y.
    \]
    
    \item Now consider the Laplace distribution, whose density 
    \[
    f(z \mid \mu, b) = \frac{1}{2b} \exp(-\frac{\abs{z-\mu}}{b}).
    \]
    As before, consider a linear regression model given by $y = \theta^T x + \epsilon$ where $\epsilon \sim \mathcal{N}(0, \sigma^2)$. Auumer a Laplace prior on this model where $\theta \sim \mathcal{L} (0, bI)$. We want to show that $\thetamap$ in this case is equivalent to the solution of linear regression with $L_1$ regularization, whose loss is specified as
    \[
    J(\theta) = \norm{X\theta - y}_2^2 + \gamma \norm{\theta}_1.
    \]

    Following the same approach, $\thetamap$ for this model is given by
    \[
    \begin{aligned}
      \thetamap = \argmin_\theta \frac{1}{2\sigma^2} \norm{y-X\theta}_2^2 + \frac{1}{b} \norm{\theta}_1.
    \end{aligned}
    \]
    Hence, this is equivalent to the solution of linear regression with $L_1$ regularization, with $\gamma = \frac{2\sigma^2}{b}$ in the above loss function $J(\theta)$.

    \textbf{Note:} closed form solution for linear regression problem with $L_1$ regularization does not exist. To optimize this, use gradient descent with a random initialization and solve it numerically.
  \end{enumerate}
\end{solution}

\begin{remark}
  \begin{itemize}
    \item Linear regression with $L_2$ regularization is also called \emph{Ridge regression}, and $L_1$ regularization is called \emph{Lasso regression}. These regularizations can be applied to any generalized linear models just as above. Regularization techniques are also called \emph{weight decay} and \emph{shrinkage}. The Gaussian and Laplace priors encourages the parameter values to be closer to their mean (i.e. zero), which results in the shrinkage effect.
    
    \item Lasso regression is known to result in sparse parameters.
  \end{itemize}
\end{remark}

\begin{problem}[Problem 4]
  \textbf{Constructing kernels}

  Choosing kernel $K(x,z) = \phi(x)^T \phi(z)$. Mercer's theorem tells us $K$ is a Mercer kernel iff for any finite set $\{\xx\}_{i=1}^m$, the square matrix $K \in \R^{m \cross m}$ whose entries are $K_{ij} = K(\xx, x^{(j)})$ is symmetric and positive semidefinite.

  Let $K_1$, $K_2$ be kernels over $\R^n \cross \R^n$, let $a \in \R^+$ be a positive real number, let $f: \R^n \to \R$ be a real-valued function, let $\phi: \R^n \to \R^d$ be a function mapping from $\R^n$ to $\R^d$, let $K_3$ be a kernel over $\R^d \cross \R^d$, and let $p$ a polynomial with \emph{positive} coefficients.

  For each $K$ below, prove or give counter examle to show whether it is necessarily a kernel.
\end{problem}

\begin{solution}
  \begin{enumerate}[(a)]
    \item $K(x,z) = K_1(x,z) + K_2(x,z)$ is a kernel. For any finite set $\{\xx\}_{i=1}^m$, the matrix $K_1$ and $K_2$ are both symmetric and PSD. Hence, for $K = K_1 + K_2$ is also symmetric and PSD.
    
    \item $K(x,z) = K_1(x,z) - K_2(x,z)$ is not necessarily a kernel. Consider matrix
    \[
    K_1 = \diag(1,1,\dots,1), \quad K_2 = \diag(2,2,\dots,2).
    \]
    Then
    \[
    K = \diag(-1,-1,\dots,-1)
    \]
    is not PSD.
    
    \item $K(x,z) = aK_1(x,z)$ is a kernel.
    
    \item $K(x,z) = -aK_1(x,z)$ is not a kernel.
    
    \item $K(x,z) = K_1(x,z)K_2(x,z)$ is a kernel. For any finite set $\{\xx\}_{i=1}^m$, the matrix $K$ will be an elementwise product of $K_1$ and $K_2$. Hence $K$ is symmetric. Now we prove $K$ is PSD. We have
    \[
    \begin{aligned}
      z^T K z &= \sum_i \sum_j z_i K_{ij} z_j \\
      &= \sum_i \sum_j z_i K_{1_{ij}} K_{2_{ij}} z_j \\
      &= \sum_i \sum_j z_i \sum_k \phi_k^1(\xx) \phi_k^1(x^{(j)}) \sum_l \phi_l^2(\xx) \phi_l^2(x^{(j)}) z_j \\
      &= \sum_k \sum_l \qty(z_i \phi_k^1(\xx) \phi_l^2(\xx))^2 \\
      &\geq 0,
    \end{aligned}
    \]
    as desired.
    
    \item $K(x,z) = f(x)f(z)$ is a kernel. For any finite set $\{\xx\}_{i=1}^m$, the matrix $K$ is cleraly symmetric. Now, we have
    \[
    \begin{aligned}
      z^T K z &= \sum_i \sum_j z_i K_{ij} z_j \\
      &= \sum_i \sum_j z_i f(\xx) f(x^{(j)}) z_j \\
      &= \qty(\sum_i z_i f(\xx))^2 \\
      & \geq 0.
    \end{aligned}
    \]
    Hence, $K$ is also PSD.

    \item $K(x,z) = K_3(\phi(x),\phi(z))$ is a kernel. Suppose the feature map corresponds to $K_3$ is $\psi$, then
    \[
    K(x,z) = (\psi \circ \phi(x))^T (\psi \circ \phi(z)).
    \]
    Hence, $K$ is a kernel corresponds to feature map $\psi \circ \phi$.

    \item $K(x,z) = p(K_1(x,z))$ is a kernel. Let $p(t) = \sum_{k} a_k t^k$. Then, $K(x,z) = \sum_k a_k K_1(x,z)^k$. By part (e), $K_1(x,z)^k$ is a kernel for each $k$. Since $a_k > 0$, $K(x,z) = \sum_k a_k K_1(x,z)^k$ is a kernel.
  \end{enumerate}
\end{solution}

\begin{problem}[Problem 5]
  \textbf{Kernelizing the Perceptron}

  Binary classification problem with $y \in \{0,1\}$. The perceptron uses hypothese of the form $h_\theta(x) = g(\theta^T x)$, where $g(z) = \sgn{z} = 1$ if $z \geq 0$, $0$ otherwise. Consider stochastic gradient descent-like implementation of perceptron algorithm, where each update to $\theta$ is made using one training example. However, unlike stochastic gradient descent, the perceptron algorithm will only make one pass through the entire training set. Update rule is
  \[
  \theta^{(i+1)} := \theta^{(i)} + \alpha \qty(y^{(i+1)} - h_{\theta^{i}}(x^{(i+1)})) x^{(i+1)},
  \]
  where $\theta^{(i)}$ is the value of parameters after the algorithm has seen the first $i$ training examples. Prior to seeing any training examples, $\theta^(0)$ is initialized to $0$.
\end{problem}

\begin{solution}
  \begin{enumerate}[(a)]
    \item Let $K$ be a Mercer kenel corresponding to some very high-dimensional feature mapping $\phi$. Suppose $\phi$ is so high-dimensional that it is infeasible to ever represent $\phi(x)$ explicitly. Descirbe how to apply "kernel trick" to the perceptron to make it work in the high-dimensional feature space $\phi$, but without ever explicitly computering $\phi(x)$.
    
    No need to worry about the intercept term. If you like, think of $\phi$ as having the property that $\phi_0(x) = 1$ so that this is taken care of. 

    \begin{enumerate}[i.]
      \item To represent the high-dimensional parameter vector $\theta$ implicitly, we will represent it as a linear combination of $\phi(\xx)$, where $\xx$ is the training examples. That is,
      \[
      \thth = \sum_k \beta_k^{(i)} \phi(\xx[k]).
      \]
      In particular, the initial value $\theta^{(0)}$ is represented by setting $\beta_k = 0$ for all $i$.

      \item To efficiently make a prediction on a new input $x^{(i+1)}$, we compute
      \[
      \begin{aligned}
        h_{\thth} (x^{(i+1)}) &= g \qty((\thth)^T \phi(x^{(i+1)})) \\
        &= g \qty( (\sum_k \beta_k^{(i)} \phi(x^{(k)}))^T \phi(x^{(i+1)})) \\
        &= g \qty( \sum_k \beta_k^{(i)} K(x^{(k)}, x^{(i+1)})).
      \end{aligned}
      \]

      \item For a new training example $(\xx[i+1], \yy[i+1])$, consider the original update rule
      \[
      \thth[i+1] := \thth + \alpha \qty(\yy[i+1] - h_{\thth}(\xx[i+1])) \xx[i+1].
      \]
      For our representation, we have
      \[
      \sum_k \beta_k^{(i+1)} \phi(\xx[k]) := \sum_k \beta_k^{(i)} \phi(\xx[k]) + \alpha \qty(\yy[i+1] - h_{\thth}(\xx[i+1])) \phi(\xx[i+1]).
      \]
      Hence, the modified update rule is
      \[
      \beta_{i+1}^{(i+1)} := \beta_{i+1}^{(i)} + \alpha \qty(\yy[i+1] - g \qty(\sum_k \beta_k^{(i)} K(\xx[k], \xx[i+1])))
      \]
      for $\beta_{i+1}$, and
      \[
      \beta_k^{(i+1)} := \beta_k^{(i)}
      \]
      for $\beta_k$ with $k \neq i+1$.
    \end{enumerate}

    \item \textbf{Coding problem.}
    \item The dot kernel performs extremely poorly, since there is no feature mapping and it is equivalent to logistic regression, while the dataset is not linearly separable.
  \end{enumerate}
\end{solution}

\begin{problem}[Problem 6]
  \textbf{Spam classification}

  Use Bayes algorithm and an SVM to build a spam classifier to detect SMS spam messages. 
\end{problem}

\begin{solution}
  \begin{enumerate}[(a)]
    \item \textbf{Coding problem.} Implement code for processing the spam messages into numpy arrays.
    
    \item \textbf{Coding problem.} Implement a naive Bayes classifier for spam classification with multinomial event model and Laplace smoothing.
    
    For multinomial event model, the model assume the probability that word $k$ appears in some location is $\phi_{k \mid y = 1}$ given $y = 1$ and $\phi_{k \mid y = 0}$ given $y = 0$. That is 
    \[
    \begin{aligned}
      \phi_{k \mid y = 1} = p(x_j = k \mid y = 1) \quad \phi_{k \mid y = 0} = p(x_j = k \mid y = 0),
    \end{aligned}
    \]
    for $k$ that index into the dictionary size $\abs{V}$ and $j$ that index into the lenght of the message. Also, the model assume the probability of a spam email $\phi_y = p(y)$. Hence, the log-likelihood for Naive Bayes algorithm
    \[
    \begin{aligned}
      \ell(\phi_y, \phi_{k \mid y = 1}, \phi_{k \mid y = 0}) &= \log \prod_{i=1}^m \qty(\prod_{j=1}^{d_i} p(x_j^{(i)} \mid y)) p(\yy) \\
      &= \sum_{i=1}^m \sum_{j=1}^{d_i} \qty (\log p(x_j^{(i)} \mid y)) + \log p(\yy).
    \end{aligned}
    \]
    and the maximum likelihood estimation with Laplace smoothing
    \[
    \begin{aligned}
      \phi_y &= \frac{\sum_{i=1}^m \ind{\yy = 1} }{m}, \\
      \phi_{k \mid y = 0} &= \frac{\sum_{i=1}^m \sum_{j=1}^{d_i} \ind{x_j^{(i)} = k \land \yy = 1} + 1}{\sum_{i=1}^m \ind {\yy = 1} d_i + \abs{V}}, \\
      \phi_{k \mid y = 0} &= \frac{\sum_{i=1}^m \sum_{j=1}^{d_i} \ind{x_j^{(i)} = k \land \yy = 0} + 1}{\sum_{i=1}^m \ind {\yy = 0} d_i + \abs{V}}. \\
    \end{aligned}
    \]
    Notice that $\sum_{j=1}^{d_i} \ind{x_j^{(i)} = k \land \yy = 1}$ is just the count of word $k$ in dictionary for message $\xx$, so we only need to count the number of each words in each message.

    To predict from the Naive Bayes model, we need to compare
    \[
    p(y = 1 \mid x) = \frac{p(x \mid y = 1)p(y = 1)}{p(x)}, \qquad p(y = 0 \mid x) = \frac{p(x \mid y = 0)p(y = 0)}{p(x)}.
    \]
    When calculating $p(x \mid y) = \prod_j p(x_j \mid y)$, underflow may happen since every term is smaller than one. We resolve this issue by using logarithm
    \[
    \begin{aligned}
      \log p(x \mid y)p(y) &= \log \prod_j p(x_j \mid y) p(y) \\
      &= \sum_j \log p(x_j \mid y) + \log p(y).
    \end{aligned}
    \]
    and compare
    \[
    \begin{aligned}
      \log p(y = 1 \mid x) &= \sum_j \log p(x_j \mid y = 1)+ \log p(y=1) - \log p(x), \\
      \log p(y = 0 \mid x) &= \sum_j \log p(x_j \mid y = 0) + \log p(y=0) - \log p(x).
    \end{aligned}
    \]
    by taking the difference.

    \item Some tokens may be particularly indicative of an SMS being in a particular class. Try to get an informal sense of how indicative token $k$ is for the SPAM class by looking at 
    \[
    \log \frac{p(x_j = k \mid y = 1)}{p(x_j = k \mid y = 0)} = \log \frac{P(\text{token } k \mid \text{email is SPAM})}{P(\text{token } k \mid \text{email is NOTSPAM})}.
    \]

    \textbf{Coding problem.}
  \end{enumerate}
\end{solution}

\end{document}
