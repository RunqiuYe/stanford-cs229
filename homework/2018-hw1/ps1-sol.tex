% HMC Math dept HW template example
% v0.04 by Eric J. Malm, 10 Mar 2005
\documentclass[12pt,letterpaper,boxed]{hmcpset}

% set 1-inch margins in the document
\usepackage[margin=1in]{geometry}
\usepackage{alltt}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathdots}
\usepackage{mathtools}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{pdflscape}
\usepackage{pgfplots}
\usepackage{siunitx}
\usepackage{slashed}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usepackage[normalem]{ulem}
\usepackage[all]{xy}
\usepackage{imakeidx}
\usepackage{enumerate}
\usepackage{physics}

% include this if you want to import graphics files with /includegraphics
\usepackage{graphicx}

\newcommand{\yy}{y^{(i)}}
\newcommand{\xx}{x^{(i)}}
\renewcommand{\tt}{t^{(i)}}
\newcommand{\ww}{w^{(i)}}
\renewcommand{\ss}{\sigma^{(i)}}


% info for header block in upper right hand corner
\name{Runqiu Ye}
\class{Stanford CS229}
\assignment{Problem Set \#1}
\duedate{06/23/2024}

\linespread{1.15}
\begin{document}

\problemlist{Problem Set \#1: Supervised Learning}

\begin{problem} [Problem 1]
    \textbf{Linear Classifiers (Logistic Regression and GDA)}
    
    Consider two datasets provided in the following files:
    \begin{enumerate}[i.]
    	\item \verb*|data/ds1_{train,valid},csv|
    	\item \verb*|data/ds2_{train,valid},csv|
    \end{enumerate}
	Each file contains $m$ examples, one example per row. The $i$-th row contains columns $\xx_0 \in \R$, $\xx_1 \in \R$ and $\yy \in \{0, 1\}$. Use logistic regression and GDA to perform binary classification. 
\end{problem}

\begin{solution}

\begin{enumerate}[(a)]
  \item Average empirical loss for logistic regression:
  \[
  J(\theta) = -\frac{1}{m} \sum_{i=1}^m \yy \log(h_\theta (\xx)) + (1-\yy)
    \log(1-h_\theta(\xx)),
  \]
  where $\yy \in \{0,1\}$, $h_\theta(\xx) = g(\theta^T x)$ and $g(z) = 1/(1+e^{-z})$.
  
  The gradient of the function
  \[
  \pdv{J}{\theta_j} = - \frac{1}{m} \sum_{i=1}^m (\yy - h_\theta(\xx)) \xx_j.
  \]
  It follows that
  \[
  \pdv{J}{\theta_k}{\theta_j} = \frac{1}{m} \sum_{i=1}^m h_\theta(\xx) (1-h_\theta(\xx)) \xx_k \xx_j.
  \]
  Hence, The Hessian $H$ of this function is
  \[
  H = \frac{1}{m} \sum_{i=1}^m  h_\theta(\xx) (1-h_\theta(\xx)) \xx (\xx)^T.
  \]
  
  Now, for any vector $z$, using Einstein's summation, we have
  \[
  \begin{aligned}
  z^T H z &= \frac{1}{m} \sum_{i=1}^m h_\theta(\xx) (1-h_\theta(\xx)) z_k \xx_k \xx_j z_j \\
  &= 
  \frac{1}{m} \sum_{i=1}^m h_\theta(\xx) (1-h_\theta(\xx)) (x^T z)^2\\
  & \geq 0
  \end{aligned}
  \]
  
  This shows that $H$ is PSD, and $J$ is convex.
  
  \item \textbf{Coding problem.}

  \item 
  To show that GDA results in a classifier that has a linear decision boundary, we want to show 
  \[
  p(y = 1 \mid x; \phi, \mu_0, \mu_1, \Sigma) = \frac{1}{1 + \exp(-(\theta^T x + \theta_0))}
  \]
  for some $\theta \in \R^n$ and $\theta_0 \in \R$ as functions of $\phi$, $\Sigma$, $\mu_0$, and $\mu_1$.
  We have
  \[
  \begin{aligned}
      p(y = 1 \mid x) &= \frac{p(x \mid y=1) p(y=1)}{p(x \mid y=1) p(y=1) + p(x\mid y = 0) p(y = 0)} \\
      &= \frac{\phi \exp(-\frac{1}{2} (x-\mu_1)^T \Sigma^{-1} (x-\mu_1) )}{\phi \exp(-\frac{1}{2} (x-\mu_1)^T \Sigma^{-1} (x-\mu_1)) + (1-\phi) \exp(-\frac{1}{2} (x-\mu_0)^T \Sigma^{-1} (x-\mu_0))} \\
      &= \frac{1}{1 + \frac{1-\phi}{\phi} \exp(-\frac{1}{2} (x-\mu_0)^T \Sigma^{-1} (x-\mu_0) + \frac{1}{2} (x-\mu_1)^T \Sigma^{-1} (x-\mu_1))} \\
      &= \frac{1}{1 +\frac{1-\phi}{\phi} \exp(-((\mu_1 - \mu_0)^T \Sigma^{-1} x + \frac{1}{2} (\mu_0^T \Sigma^{-1} \mu_0 - \mu_1^T \Sigma^{-1} \mu_1)))}.
  \end{aligned}
  \]
  This is the desired form, where
  \[
  \begin{aligned}
      \theta &= \Sigma^{-1} (\mu_1 - \mu_0), \\
      \theta_0 &= \frac{1}{2} (\mu_0^T \Sigma^{-1} \mu_0 - \mu_1^T \Sigma^{-1} \mu_1) - \log \frac{1-\phi}{\phi}.
  \end{aligned}
  \]
  
  \item The log-likelihood of the data is
  \[
  \begin{aligned}
      \ell(\phi, \mu_0, \mu_1, \Sigma) &= \log \prod_{i=1}^m p(\xx \mid \yy; \mu_0, \mu_1, \Sigma) p(\yy; \phi) \\
      &= \sum_{i=1}^m 1\{\yy=1\} \qty(-\frac{1}{2} (\xx-\mu_1)^T \Sigma^{-1} (\xx-\mu_1) + \log \phi) \\
      & \qquad + \sum_{i=1}^m 1\{\yy=0\} \qty(-\frac{1}{2} (\xx-\mu_0)^T \Sigma^{-1} (\xx-\mu_0) + \log (1-\phi))  \\
      & \qquad - \frac{m}{2} \log \abs{\Sigma} +  C,
  \end{aligned}
  \]
  where $C$ is some constant independent of the parameters.
  
  Let $\nabla_\phi \ell = 0$, we have
  \[
  \phi = \frac{1}{m} \sum_{i=1}^m 1\{\yy=1\}.
  \]
  Let $\nabla_{\mu_1} \ell = 0$, we have
  \[
  \sum_{i=1}^m 1\{\yy=1\} \Sigma^{-1}\xx = \sum_{i=1}^m 1\{\yy=1\} \Sigma^{-1} \mu_1,
  \]
  and thus
  \[
  \mu_1 = \frac{\sum_{i=1}^m 1\{\yy=1\} \xx}{\sum_{i=1}^m 1\{\yy=1\}}, \quad 
  \mu_0 = \frac{\sum_{i=1}^m 1\{\yy=0\} \xx}{\sum_{i=1}^m 1\{\yy=0\}}.
  \]
  To derive $\Sigma$, recall that $\nabla_{A} \log \abs{A} = (A^{-1})^T$, so we have
  \[
  \nabla_{\Sigma^{-1}} \ell = -\frac{m}{2} \Sigma^{-1} + \frac{1}{2}\sum_{i=1}^m (\xx - \mu_{\yy})(\xx-\mu_{\yy})^T.
  \]
  Hence,
  \[
  \Sigma = \frac{1}{m} \sum_{i=1}^m (\xx - \mu_{\yy})(\xx-\mu_{\yy})^T.
  \]
  
  We conclude that the maximum likelihood estimates of the parameters are given by
  \[
  \begin{aligned}
  	\phi &= \frac{1}{m} \sum_{i=1}^m 1\{\yy = 1\}, \\
  	\mu_0 &= \frac{\sum_{i=1}^{m} 1\{\yy = 0\} \xx }{\sum_{i=1}^m 1\{\yy = 0\}}, \\
  	\mu_1 &= \frac{\sum_{i=1}^{m} 1\{\yy = 1\} \xx }{\sum_{i=1}^m 1\{\yy = 1\}} ,\\ 
  	\Sigma &= \frac{1}{m} \sum_{i=1}^m (\xx - \mu_{\yy})(\xx - \mu_{iy})^T.
  \end{aligned}
  \]
    
  \item \textbf{Coding problem.}
  
  \item See jupyter notebook for plots.
  
  \item See jupyter notebook for plots. On Dataset 1 GDA perform worse than logistic regression. This might be the case because for Dataset 1, the distribution of features are not quite multivariate normal.
  
  \item *** TO-DO ***
    
\end{enumerate}
\end{solution}

\begin{problem}[Problem 2]
	\textbf{Incomplete, Positive-Only Labels}
	
	Dataset without full access to labels. In particular, we have labels only for a subset of positive examples. All negative examples and the rest of positive examples are unlabeled.
	
	Assume dataset $\{ (\xx, \tt, \yy) \}_{i=1}^m$ where $\tt \in \{0,1\}$ is true label and where 
	\[
	\yy = \begin{cases}
		1 & \xx \text{ is labeled}\\
		0 & \text{otherwise.}
	\end{cases}
	\]
	All labeled examples are positive, which is to say $p(\tt = 1 \mid \yy = 1) = 1$. Goal is to construct a binary classifier $h$ of true label $t$ which only access to partial labels $y$. That is, construct $h$ such that $h(\xx) \approx p(\tt = 1 \mid \xx)$ as closely as possible, using only $x$ and $y$.
\end{problem}

\begin{solution}
\begin{enumerate}[(a)]
	\item Suppose each $\yy$ and $\xx$ conditionally independent given $\tt$:
	\[
	p(\yy = 1 \mid \tt = 1, \xx) = p(\yy = 1 \mid \tt = 1).
	\]
	That is, labeled examples are selected uniformly at random from positive examples.
	
	Want to show $p(\tt = 1 \mid \xx) = p(\yy = 1 \mid \xx) / \alpha$ for some $\alpha \in \R$. As $p(\cdot \mid \xx)$ is a conditional measure, we have
	\[
	\begin{aligned}
		p(\yy = 1 \mid \xx) &= p(\yy = 1 \mid \tt = 1, \xx) p(\tt = 1 \mid \xx) \\
		& \qquad + p(\yy = 1 \mid \tt = 0, \xx) p(\tt = 0 \mid \xx) \\
		&= p(\yy = 1 \mid \tt = 1, \xx) p(\tt = 1 \mid \xx) \\
		&= p(\yy = 1 \mid \tt = 1) p(\tt = 1 \mid \xx).
	\end{aligned}
	\]
	Hence, $p(\tt = 1 \mid \xx) = p(\yy = 1 \mid \xx) / \alpha$ where $\alpha = p(\yy = 1 \mid \tt = 1)$.
	
	\item Estimate $\alpha$ using a trained classifier $h$ and a held-out validation set $V$. Let $V_+ = \{\xx \in V \mid \yy = 1\}$. Assuming $h(\xx) \approx p(\yy = 1 \mid \xx)$ for all $\xx$. Want to show
	\[
	h(\xx) \approx \alpha \text{ for all } \xx \in V_+.
	\]
	May assume that $p(\tt = 1 \mid \xx) \approx 1$ when $\xx \in V_+$.
	
	We have
	\[
	\begin{aligned}
		h(\xx) &\approx p(\yy = 1 \mid \xx) \\
		&= p(\yy = 1 \mid \tt = 1, \xx) p(\tt = 1 \mid \xx) \\
		&\approx \alpha.
	\end{aligned}
	\]
	
	\item \textbf{Coding problem.}
	
	\item \textbf{Coding problem.}
	
	\item \textbf{Coding problem.} Estimate the constant $\alpha$ using validation set.
  \[
  \alpha \approx \frac{1}{\abs{V_+}} \sum_{\xx \in V_+} h(\xx).
  \]
  To plot the decision boundary, we need to calculate the rescaled $\theta$, write $\theta_*$. The new decision boundary is given by $\frac{1}{\alpha} \frac{1}{1+ \exp(-\theta^T x)} = \frac{1}{2}$. We have
  \[
  \theta^T x + \log(\frac{2}{\alpha} - 1) = 0.
  \]
  This is equivalent to $\theta_*^T x = 0$. This shows that $\theta_*$ and $\theta$ differs only in the 0-th index by a constant $\log(\frac{2}{\alpha} - 1)$.
\end{enumerate}
\end{solution}

\begin{problem}[Problem 3]
  \textbf{Poisson Regression}
\end{problem}
\begin{solution}
\begin{enumerate}[(a)]
  \item The poisson distribution parametrized by $\lambda$ is 
  \[
  p(y ; \lambda) = \frac{e^{-\lambda} \lambda^y}{y!}.
  \]
  Therefore, we have
  \[
    p(y ; \lambda) = \frac{1}{y!} \exp(-\lambda + y \log \lambda).
  \]
  Compare with $p(y ; \eta) = b(y) \exp (\eta^T T(y) - a(\eta))$, we conclude that the poisson distribution is in the exponential family, with
  \[
  \begin{aligned}
    b(y) &= \frac{1}{y!}, \\
    T(y) &= y, \\
    \eta &= \log \lambda, \\
    a(\eta) &= e^\eta.
  \end{aligned}
  \]

  \item The canonical response function for the family
  \[
  \E[T(y); \eta] = \E[T(y); \eta] = \lambda = e^\eta.
  \]

  \item For a general linear model and a training set, the log likelihood
  \[
  \begin{aligned}
    \log p(\yy \mid \xx; \eta) &= \log b(y) \exp(\eta^T T(y) - a(\eta)) \\
    &= \log b(y) + \eta^T T(y) - a(\eta).
  \end{aligned}
  \]
  
  For our model with poisson responses y, we have
  \[
  \begin{aligned}
    \ell = \log p(\yy \mid \xx; \theta) &= - \log y! + (\theta^T \xx) \yy - \exp(\theta^T \xx).
  \end{aligned}
  \]

  Taking the derivative with respect to $\theta_j$, we have
  \[
  \pdv{\ell}{\theta_j} = \qty(\yy - \exp(\theta^T \xx)) \xx_j
  \]

  Hence, the stochastic gradient ascent update rule for learning using a GLM model with poisson response $y$ is 
  \[
  \begin{aligned}
    \theta_j &:= \theta_j + \alpha \pdv{\ell}{\theta_j} \\
    &:= \theta_j + \alpha \qty(\yy - \exp(\theta^T \xx)) \xx_j.
  \end{aligned}
  \]

  \item \textbf{Coding problem.} To predict the dataset, recall that the hypothese function for our model with poisson response $y$ is
  \[
  h_\theta(x) = \E[y \mid x] = e^{\eta} = e^{\theta^T x}.
  \]
  Also, for the model, we utilize batch gradient ascent:
  \[
  \theta_j := \theta_j + \frac{\alpha}{m} \sum_{i=1}^{m}  \qty(\yy - \exp(\theta^T \xx)) \xx_j.
  \]

\end{enumerate}
\end{solution}

\begin{problem}[Problem 4]
  \textbf{Convexity of Generalized Linear Models} 
  
  Investigate nice properties of GLM. Goal is to show that the negative log-likelihood (NLL) loss of a GLM is convex with respect to the model parameters.

  Recall that for exponential family distribution
  \[
  p(y ; \eta) = b(y) \exp(\eta^T T(y) - a(\eta)),
  \]
  where $\eta$ is the \emph{natural parameter} of distribution. Our approach is to show the Hessian of loss w.r.t the model parameters is PSD.

  Restrict to the case where $\eta$ is scalar and $\eta$ is modeled as $\theta^T x$. Assume $p(Y \mid X ; \theta) \sim \text{ExponentialFamily}(\eta)$ where $\eta \in \R$ is a scalar and $T(y) = y$. That is
  \[
  p(y ; \eta) = b(y) \exp(\eta y - a(\eta)).
  \]
\end{problem}

\begin{solution}
  \begin{enumerate}[(a)]
    \item The mean of the distribution
    \[
    \E[y; \eta] = \int y p(y ; \eta) dy = \int y b(y) \exp(\eta y - a(\eta)) dy.
    \]
    Following the hint, observe that
    \[
    \begin{aligned}
      \pd{\eta} \int p(y;\eta) dy &= \int \pd{\eta} p(y ; \eta) dy \\ 
      &= \int b(y) \qty(y - \pdv{a}{\eta}) \exp(\eta y - a(\eta)) dy.
    \end{aligned}
    \]
    While $\int p(y; \eta) dy = 1$, we have $\pd{\eta} \int p(y; \eta) = 0$ and 
    \[
    \E[y ; \eta] = \int b(y) \pdv{a(\eta)}{\eta} \exp(\eta y - a(\eta)) dy.
    \]
    Since $\pdv{a(\eta)}{\eta}$ does not depend on $y$, we have
    \[
    \E[y ;\eta] = \int b(y) \pdv{a(\eta)}{\eta} \exp(\eta y - a(\eta)) dy = \pdv{a(\eta)}{\eta} \int b(y) \exp(\eta y - a(\eta)) dy = \pdv{a(\eta)}{\eta}.
    \]
    This shows that $\E[Y \mid X; \theta]$ can be represented as the gradient of the log-partition function $a$ with respect to the natural parameter $\eta$.

    \item Notice that
    \[
    \begin{aligned}
      \pdv{\E[y; \eta]}{\eta} &= 
      \pd{\eta} \int y b(y) \exp(\eta y - a(\eta)) dy \\
      &= \int y b(y) \qty(y - \pdv{a}{\eta}) \exp(\eta y - a(\eta)) dy \\
      &= \int y^2 b(y) \exp(\eta y - a(\eta)) dy - \int y b(y) \pdv{a}{\eta} \exp(\eta y - a(\eta)) dy \\
      &= \E[y^2; \eta] - \pdv{a}{\eta} \E[y; \eta] \\
      &= \E[y^2; \eta] - (\E[y; \eta])^2 \\
      &= \Var(y; \eta).
    \end{aligned}
    \]
    This completes the proof, and we can see that $\Var(Y \mid X ; \theta)$ can be expressed as the second derivative of the mean w.r.t $\eta$ (i.e. the second derivative of log-partition function $a(\eta)$ w.r.t natural parameter $\eta$).

    \item The loss function $\ell(\theta)$, the NLL of the distribution
    \[
    \begin{aligned}
      \ell(\theta) &= - \log \prod_{i=1}^m p(\yy \mid \xx ; \eta) \\
      &= - \sum_{i=1}^{m} \log p(\yy \mid \xx ; \eta) \\
      &= \sum_{i=1}^{m} - \log b(\yy) - \eta \yy + a(\eta) \\
      &= \sum_{i=1}^{m} - \log b(\yy) - \yy \theta^T \xx + a(\theta^T \xx).
    \end{aligned}
    \]
    Now, to calculate the Hessian of the loss function w.r.t $\theta$, we first calculate
    \[
    \begin{aligned}
      \pdv{\ell}{\theta_k} = \sum_{i=1}^m \qty(\pdv{a}{\eta} - \yy) \xx_k.
    \end{aligned}
    \]
    It follows that
    \[
    \pdv{\ell}{\theta_j \theta_k} = \sum_{i=1}^m \pdv[2]{a}{\eta} \xx_j \xx_k.
    \]
    Hence, the Hessian of the loss function is 
    \[
    H = \sum_{i=1}^m \pdv[2]{a}{\eta} \xx (\xx)^T.
    \]

    To prove the Hessian is always PSD, consider any $z \in \R^n$, where $n$ is the dimension of $\xx$, and
    \[
    \begin{aligned}
      z^T H z &= \sum_{i=1}^m z_j H_{jk} z_k \\
      &= \sum_{i=1}^m \pdv[2]{a}{\eta} z_j x_j x_k z_k \\
      &= \sum_{i=1}^m \Var(Y \mid X; \eta) (x^T z)^2 \\
      &\geq 0,
    \end{aligned}
    \]
    since the variance is always non-negative. This completes the proof that NLL loss of GLM is convex.
  \end{enumerate}
\end{solution}

\begin{remark}
  \begin{itemize}
    \item Any GLM model is \emph{convex} in its model parameters.
    \item The exponential family of probability distribution are mathematically nice. We can caucluate the means and variance using derivatives, which is easier that integrals.
  \end{itemize}
\end{remark}

\begin{problem}[Problem 5]
  \textbf{Locally weighted linear regression}
\end{problem}
\begin{solution}
\begin{enumerate}[(a)]
  \item Weighted linear regression. Specifically, want to minimize
  \[
  J(\theta) = \frac{1}{2} \sum_{i=1}^m \ww (\theta^T \xx - \yy)^2
  \]
  \begin{enumerate}[i.]
    \item Let $X$ be the $m$ by $n$ matrix where the $i$-th row is $(\xx)^T$, and let $y$ be the $m$ by $1$ matrix where the $i$-the row is $\yy$. Then $J$ can also be written
    \[
    J(\theta) = (X\theta - y)^T W (X \theta - y),
    \]
    where $W$ is the $m$-by-$m$ diagonal matrix
    \[
    W_{ij} = \frac{1}{2} \delta_{ij} \ww.
    \]

    \item If all the $\ww$ is 1, then the normal equation is
    \[
    X^T X \theta = X^T y,
    \]
    and the value of $\theta$ that minimizes $J(\theta)$ is given by $(X^T X)^{-1} X^T y$. Here, to genrealize the normal equation, we first calculate the derivative
    \[
    \nabla_{\theta} J = X^T (2W(X \theta - y)) = 2X^T W X \theta - 2X^T W y.
    \]
    Setting this to 0, we get the normal equation
    \[
    X^T W X \theta = X^T W y
    \]
    and the expression for $\theta$
    \[
    \theta = (X^T W X)^{-1} X^T W y.
    \]
    Notice that for $\ww = 1$, $W = I$ and we get the original form of the normal equation.

    \item Suppose dataset $\{(\xx, \yy)\}_{i=1}^m$ of $m$ independent examples, but we model $\yy$ as drawn from conditional distributions with different levels of variance $(\ss)^2$. Specifically, assume the model
    \[
    p(\yy \mid \xx ; \theta) = \frac{1}{\sqrt{2\pi} \ss} \exp(-\frac{(\yy - \theta^T \xx)^2}{2(\ss)^2}).
    \]
    We want to show finding the maximum likelihood estimates of $\theta$ reduces to solving a weighted linear regression problem.

    The log-likelihood
    \[
    \begin{aligned}
      \ell (\theta) &= \log \prod_{i=1}^m p(\yy \mid \xx; \theta) \\
      &= \sum_{i=1}^{m} -\log \sqrt{2\pi} \ss - \frac{(\yy - \theta^T \xx)^2}{2(\ss)^2}.
    \end{aligned}
    \]
    To find the maximum likelihood estimate of $\theta$, we need to minimize
    \[
    J(\theta) = \sum_{i=1}^{m} \frac{(\yy - \theta^T \xx)^2}{2(\ss)^2}.
    \]
    Hence this is equivalent to solving a weighted linear regression problem, where
    \[
    \ww = \frac{1}{(\ss)^2}.
    \]
  \end{enumerate}

  \item \textbf{Coding problem.} Implement locally weighted linear regression using normal equation in Part (a) and using
  \[
  \ww = \exp(- \frac{\norm{\xx - x}_2^2}{2 \tau^2}).
  \]
  The model seems to be \emph{underfitting} for \verb|data/ds5_{train,valid,test}.csv|.

  \item \textbf{Coding problem.} Find the best hyperparameter $\tau$ that achieves the lowest MSE on the valid set.
\end{enumerate}
\end{solution}

\end{document}
