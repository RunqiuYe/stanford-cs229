% HMC Math dept HW template example
% v0.04 by Eric J. Malm, 10 Mar 2005
\documentclass[12pt,letterpaper,boxed]{hmcpset}

% set 1-inch margins in the document
\usepackage[margin=1in]{geometry}
\usepackage{alltt}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathdots}
\usepackage{mathtools}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{pdflscape}
\usepackage{pgfplots}
\usepackage{siunitx}
\usepackage{slashed}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usepackage[normalem]{ulem}
\usepackage[all]{xy}
\usepackage{imakeidx}
\usepackage{enumerate}
\usepackage{physics}

% include this if you want to import graphics files with /includegraphics
\usepackage{graphicx}

\renewcommand{\tt}{t^{(i)}}
\newcommand{\ww}{w^{(i)}}
\renewcommand{\ss}{\sigma^{(i)}}
\newcommand{\ind}[1]{\mathbb{I}\{#1\}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\thetamap}{\theta_{\mathrm{MAP}}}
\renewcommand{\diag}{\mathrm{diag}}
\newcommand{\DKL}[2]{D_{\mathrm{KL}}(#1 \parallel #2)}
\newcommand{\lunsup}{\ell_{\mathrm{unsup}}}
\newcommand{\lsup}{\ell_{\mathrm{sup}}}
\newcommand{\lsemi}{\ell_{\mathrm{semi-sup}}}
\newcommand{\tht}{\theta^{(t)}}
\newcommand{\thtt}{\theta^{(t+1)}}
\newcommand{\summ}{\sum_{i=1}^m}
\newcommand{\sumtm}{\sum_{i=1}^{\tilde m}}

% info for header block in upper right hand corner
\name{Runqiu Ye}
\class{Stanford CS299}
\assignment{Problem Set \#4}
\duedate{08/07/2024}

\linespread{1.15}
\begin{document}

\problemlist{Problem Set \#3: EM, Deep Learning, \& Reinforcement Learning}

\begin{problem}[Problem 1]
\textbf{Neural Networks: MNIST image classification}

Implement a simple convolutional neural network to classify grayscale images of handwritten digits from the MNIST dataset. The starter code splits the set of 60000 training images and labels into a sets of 59600 examples as the training set and 400 examples for the dev set.

To start, implement convolutional neural network and cross entropy loss, and train it with the provided dataset. The architecture is as follows:
\begin{enumerate}[(a)]
  \item The first layer is a convolutional layer with 2 output channels with a convolution size of 4 by 4.
  \item The second layer is a max pooling layer of stride and width 5 by 5.
  \item The third layer is a ReLU activation layer.
  \item After the four layer, the data is flattened into a single dimension.
  \item The fifth layer is a single linear layer with output size 10 (the number of classes).
  \item The sixth layer is a softmax layer that computes the probabilities for each classes.
  \item Finally, we use a cross entropy loss as our loss function.
\end{enumerate}

The cross entropy loss is
\[
CE(y, \hat y) = - \sum_{k=1}^K y_k \log \hat{y}_k,
\]
where $\hat y \in \R^K$ is the vector of softmax outputs from the model for the training example $x$, and $y \in \R^K$ is the ground-truth vector for the training example $X$ such that $y = [0, \dots, 0, 1, 0, \dots, 0]^T$ contains a single 1 at the position of the correct classes.

We also use mini-batch gradient descent with a batch size of 16. Normally we would iterate over the data multiple times with multiple epochs, but for this assignment we only do 400 batches to save time.
\end{problem}

\begin{solution}
\begin{enumerate}[(a)]
  \item Implement functions within \verb|p01_nn.py|.
  \item Implement a function that computes the full backward pass.
\end{enumerate}
\end{solution}

\begin{problem}[Problem 2]
  \textbf{Off Policy Evaluatino And Causal Inference}

  Need methods for evaluating policies without actually implementing them. This task is off-policy evaluation or causal inference.

  For this problem, consider MDPs with a single timestep. Consider universe of states $S$, actions $A$, a reward function $R(s, a)$ where $s$ is a state and $a$ is an action. We often only have a subset of $a$ in our dataset. For eaxmple, each state $s$ could represent a patient, each action $a$ could represent which drug we prescribe to that patient and $R(s, a)$ be their lifespan after prescribing that drug.

  A policy is defined as $\pi_i (s, a) = p(a \mid s, \pi_i)$ We are given observational dataset of $(s, a, R(s, a))$ tuples. Let $p(s)$ be the density for the distribution of the state $s$ values wihin the dataset. Let $\pi_0 (s, a) = p(a \mid s)$ within our observational data. $\pi_0$ corresponds to the baseline policy in observational data. Also given target policy $\pi_1 (s,a)$ which gives the conditional probability $p(a \mid s)$ in our optimal policy. Our goal is to computer expected value of $R(s,a)$ in the same population as our observatinoal data, but with a policy of $\pi_1$ instead of $\pi_0$. In other words, we are trying to compute
  \[
  \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} R(s, a) = \sum_{(s,a)} R(s,a) p(s,a) = \sum_{(s,a)} R(s,a) p(s) p(a \mid s) = \sum_{(s,a)} R(s,a) p(s) \pi_1 (s,a).
  \]

  \textbf{Notation and Simplifying Assumptions: } \\
  We will assume that each action has a non-zero probability in the observed policy $\pi_0 (s, a)$. In other words, for all actions $a$ and states $s$, $\pi_0 (s,a) > 0$.

  \textbf{Regression:} The simplest possible estimator is to directly use our learned MDP parameters to estimate our goal. This is called regression estimator. While training our MDP, we laernan estimator $\hat R(s,a)$ that estiamtes $R(s,a)$. We can now directly estimate
  \[
  \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} R(s, a)
  \]
  with 
  \[
    \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} \hat R(s, a).
  \]
  If $\hat R(s,a) = R(s,a)$, then this estimator is trivially correct. We will now consider alternative approaches and explore why you might use one estimator over another.
\end{problem}

\begin{solution}
  \begin{enumerate}[(a)]
    \item \textbf{Important Sampling:} Let $\hat \pi_0$ be an estimate of the true $\pi_0$. The importance sampling estimator uses that $\hat \pi_0$ and has the form 
    \[
    \E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}} \frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} R(s,a).
    \]
    We now show that if $\hat \pi_0 = \pi_0$, then the importance sampling estimator is equal to 
    \[
    \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} R(s,a).
    \]
    
    \begin{proof}
      
    \end{proof}
  \end{enumerate}
\end{solution}

\end{document}