% HMC Math dept HW template example
% v0.04 by Eric J. Malm, 10 Mar 2005
\documentclass[12pt,letterpaper,boxed]{hmcpset}

% set 1-inch margins in the document
\usepackage[margin=1in]{geometry}
\usepackage{alltt}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathdots}
\usepackage{mathtools}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{pdflscape}
\usepackage{pgfplots}
\usepackage{siunitx}
\usepackage{slashed}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usepackage[normalem]{ulem}
\usepackage[all]{xy}
\usepackage{imakeidx}
\usepackage{enumerate}
\usepackage{physics}

% include this if you want to import graphics files with /includegraphics
\usepackage{graphicx}

\renewcommand{\tt}{t^{(i)}}
\newcommand{\ww}{w^{(i)}}
\renewcommand{\ss}{\sigma^{(i)}}
\newcommand{\ind}[1]{\mathbb{I}\{#1\}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\thetamap}{\theta_{\mathrm{MAP}}}
\renewcommand{\diag}{\mathrm{diag}}
\newcommand{\DKL}[2]{D_{\mathrm{KL}}(#1 \parallel #2)}
\newcommand{\lunsup}{\ell_{\mathrm{unsup}}}
\newcommand{\lsup}{\ell_{\mathrm{sup}}}
\newcommand{\lsemi}{\ell_{\mathrm{semi-sup}}}
\newcommand{\tht}{\theta^{(t)}}
\newcommand{\thtt}{\theta^{(t+1)}}
\newcommand{\summ}{\sum_{i=1}^m}
\newcommand{\sumtm}{\sum_{i=1}^{\tilde m}}
\newcommand{\sumsa}{\sum_{(s,a)}}

% info for header block in upper right hand corner
\name{Runqiu Ye}
\class{Stanford CS299}
\assignment{Problem Set \#4}
\duedate{08/07/2024}

\linespread{1.15}
\begin{document}

\problemlist{Problem Set \#3: EM, Deep Learning, \& Reinforcement Learning}

\begin{problem}[Problem 1]
\textbf{Neural Networks: MNIST image classification}

Implement a simple convolutional neural network to classify grayscale images of handwritten digits from the MNIST dataset. The starter code splits the set of 60000 training images and labels into a sets of 59600 examples as the training set and 400 examples for the dev set.

To start, implement convolutional neural network and cross entropy loss, and train it with the provided dataset. The architecture is as follows:
\begin{enumerate}[(a)]
  \item The first layer is a convolutional layer with 2 output channels with a convolution size of 4 by 4.
  \item The second layer is a max pooling layer of stride and width 5 by 5.
  \item The third layer is a ReLU activation layer.
  \item After the four layer, the data is flattened into a single dimension.
  \item The fifth layer is a single linear layer with output size 10 (the number of classes).
  \item The sixth layer is a softmax layer that computes the probabilities for each classes.
  \item Finally, we use a cross entropy loss as our loss function.
\end{enumerate}

The cross entropy loss is
\[
CE(y, \hat y) = - \sum_{k=1}^K y_k \log \hat{y}_k,
\]
where $\hat y \in \R^K$ is the vector of softmax outputs from the model for the training example $x$, and $y \in \R^K$ is the ground-truth vector for the training example $X$ such that $y = [0, \dots, 0, 1, 0, \dots, 0]^T$ contains a single 1 at the position of the correct classes.

We also use mini-batch gradient descent with a batch size of 16. Normally we would iterate over the data multiple times with multiple epochs, but for this assignment we only do 400 batches to save time.
\end{problem}

\begin{solution}
\begin{enumerate}[(a)]
  \item Implement functions within \verb|p01_nn.py|.
  \item Implement a function that computes the full backward pass.
\end{enumerate}
\end{solution}

\begin{problem}[Problem 2]
  \textbf{Off Policy Evaluatino And Causal Inference}

  Need methods for evaluating policies without actually implementing them. This task is off-policy evaluation or causal inference.

  For this problem, consider MDPs with a single timestep. Consider universe of states $S$, actions $A$, a reward function $R(s, a)$ where $s$ is a state and $a$ is an action. We often only have a subset of $a$ in our dataset. For eaxmple, each state $s$ could represent a patient, each action $a$ could represent which drug we prescribe to that patient and $R(s, a)$ be their lifespan after prescribing that drug.

  A policy is defined as $\pi_i (s, a) = p(a \mid s, \pi_i)$ We are given observational dataset of $(s, a, R(s, a))$ tuples. Let $p(s)$ be the density for the distribution of the state $s$ values wihin the dataset. Let $\pi_0 (s, a) = p(a \mid s)$ within our observational data. $\pi_0$ corresponds to the baseline policy in observational data. Also given target policy $\pi_1 (s,a)$ which gives the conditional probability $p(a \mid s)$ in our optimal policy. Our goal is to computer expected value of $R(s,a)$ in the same population as our observatinoal data, but with a policy of $\pi_1$ instead of $\pi_0$. In other words, we are trying to compute
  \[
  \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} [R(s, a)] = \sum_{(s,a)} R(s,a) p(s,a) = \sum_{(s,a)} R(s,a) p(s) p(a \mid s) = \sum_{(s,a)} R(s,a) p(s) \pi_1 (s,a).
  \]

  \textbf{Simplifying Assumptions: } We will assume that each action has a non-zero probability in the observed policy $\pi_0 (s, a)$. In other words, for all actions $a$ and states $s$, $\pi_0 (s,a) > 0$.

  \textbf{Regression:} The simplest possible estimator is to directly use our learned MDP parameters to estimate our goal. This is called regression estimator. While training our MDP, we laern an estimator $\hat R(s,a)$ that estimates $R(s,a)$. We can now directly estimate
  \[
  \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} [R(s, a)]
  \]
  with 
  \[
    \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} [\hat R(s, a)].
  \]
  If $\hat R(s,a) = R(s,a)$, then this estimator is trivially correct. We will now consider alternative approaches and explore why you might use one estimator over another.
\end{problem}

\begin{solution}
  \begin{enumerate}[(a)]
    \item \textbf{Importance Sampling:} Let $\hat \pi_0$ be an estimate of the true $\pi_0$. The \emph{importance sampling estimator} uses that $\hat \pi_0$ and has the form 
    \[
    \E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}} \qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} R(s,a)].
    \]
    We now show that if $\hat \pi_0 = \pi_0$, then the importance sampling estimator is equal to 
    \[
    \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} [R(s,a)].
    \]
    
    \begin{proof}
      If $\hat \pi_0 = \pi_0$, then
      \[
      \begin{aligned}
        \E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}} \qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} R(s,a)] &= 
        \sum_{(s,a)} \frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} R(s,a) p(s) \pi_0 (s,a) \\
        &= \sum_{(s,a)} \pi_1(s,a) R(s,a) p(s) \\
        &= \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} [R(s,a)],
      \end{aligned}
      \]
      as desired.
    \end{proof}
    Note that this estimator only requires us to model $\pi_0$ as we have the $R(s,a)$ values in the observational data.

    \item \textbf{Weighted Importance Sampling.} A variant of the importance sampling estimator is the \emph{weighted importance sampling estimator}. The weighted importance sampling estimator has the from
    \[
    \frac{\E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}}\qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} R(s,a)] }{ \E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}}\qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)}] }.
    \]
    We now show that if $\hat \pi_0 = \pi_0$, then the weighted importance sampling estimator is equal to 
    \[
    \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} \qty[R(s,a)].
    \]

    \begin{proof}
      We already showed that if $\hat \pi_0 = \pi_0$, then 
      \[
      \E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}}\qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} R(s,a)]  =  \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} [R(s,a)].
      \]
      For the denominator, if $\hat \pi_0 = \pi_0$, we have
      \[
      \begin{aligned}
        \E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}}\qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)}] &= \sumsa \frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} p(s) \pi_0(s,a) \\
        &= \sumsa \pi_1(s,a) p(s) \\
        &= \sumsa p(s,a \mid \pi_1) \\
        &= 1.
      \end{aligned}
      \]
      Hence,
      \[
        \frac{\E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}}\qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} R(s,a)] }{ \E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}}\qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)}] } = \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} \qty[R(s,a)]
      \]
      when $\hat \pi_0 = \pi_0$, as desired.
    \end{proof}

    \item One issue with the weighted importance sampling estimator is that it can be biased in many finite smaple situations. In finite samples, we replace the expected value with a sum over the seen values in our observational dataset. Please show that the weighted importance sampling estimator is biased in these situations.
    
    \textbf{Hint:} Consider the case where there is only a single data element in the observational dataset.

    \begin{proof}
      When there is only a single data element $(s^*, a^*, R(s^*,a^*))$ in the observational dataset, we have
      \[
      \begin{aligned}
        & \E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}}\qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} R(s,a)] = \frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} R(s^*,a^*), \\
        & \E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}} \qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)}] = \frac{\pi_1(s,a)}{\hat \pi_0 (s,a)}.
      \end{aligned}
      \]
      The weighted importance estimator then gives
      \[
        \frac{\E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}}\qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} R(s,a)] }{ \E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}}\qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)}] } = R(s^*,a^*) = \E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}} [R(s,a)].
      \]
      If $\pi_1 \neq \pi_0$, that is, if $pi_1$ has nonzero probability taking any action $a \neq a^*$ at state $s^*$, then the weighted importance sampling estimator
      \[
        \frac{\E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}}\qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} R(s,a)] }{ \E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}}\qty[\frac{\pi_1(s,a)}{\hat \pi_0 (s,a)}] } = \E_{\substack{s \sim p(s) \\ a \sim \pi_0 (s,a)}} [R(s,a)] \neq \E_{\substack{s \sim p(s) \\ a \sim \pi_1 (s,a)}} [R(s,a)],
      \]
      causing a bias.
    \end{proof}

    \item \textbf{Doulby Robust.} One final commonly used estimator is the doubly robust estimator. The doubly robust estimator has the form 
      \[
      \E_{\substack{s \sim p(s) \\ a \sim \pi_0(s,a)}} \qty[ \E_{a \sim \pi_1(s,a)} \qty[\hat R(s,a)] + \frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} (R(s,a) - \hat R(s,a)) ].
      \]
      One advantage of the doubly robust estimator is that it works if either $\hat \pi_0 = \pi_0$ or $\hat R(s,a) = R(s,a)$.
      \begin{enumerate}[(i)]
        \item First we show that the doubly robust estimator is equal to $\E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} [R(s,a)]$ when $\hat \pi_0 = \pi_0$.
        
        \begin{proof}
          First of all, we have
          \[
          \E_{a \sim \pi_1(s,a)} [\hat R(s,a)] = \sum_{a} \pi_1(s,a) \hat R(s,a).
          \]
          Note that this expression is independent of $a$. Hence,
          \[
          \E_{\substack{s \sim p(s) \\ a \sim \pi_0(s,a)}} \qty[\E_{a \sim \pi_1(s,a)} \qty[\hat R(s,a)]] = \sum_s \sum_a \pi_1(s,a) \hat R(s,a) p(s) = \sumsa \pi_1(s,a) \hat R(s,a) p(s).
          \]
          For the second term, when $\hat \pi_0 = \pi_0$, we have
          \[
          \begin{aligned}
            \E_{\substack{s \sim p(s) \\ a \sim \pi_0(s,a)}} \frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} (R(s,a) - \hat R(s,a)) 
            &= \sumsa \frac{\pi_1(s,a)}{\hat \pi_0(s,a)} (R(s,a) - \hat R(s,a)) p(s) \pi_0(s,a) \\
            &= \sumsa \pi_1(s,a) R(s,a) p(s) - \pi_1(s,a) \hat R(s,a) p(s) \\
            &= \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} [R(s,a)] - \sumsa \pi_(s,a) \hat R(s,a) p(s).
          \end{aligned}
          \]
          Therefore, the doubly robust estimator 
          \[
          \E_{\substack{s \sim p(s) \\ a \sim \pi_0(s,a)}} \qty[ \E_{a \sim \pi_1(s,a)} \qty[\hat R(s,a)] + \frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} (R(s,a) - \hat R(s,a)) ] = \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} [R(s,a)].
          \]
          when $\hat \pi_0 = \pi_0$.
        \end{proof}

        \item Now we show the the doubly robust estimator is equal to $\E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} [R(s,a)]$ when $\hat R(s,a) = R(s,a)$.
        
        \begin{proof}
          As in the previous proof, we have
          \[
          \E_{a \sim \pi_1(s,a)} [\hat R(s,a)] = \sum_{a} \pi_1(s,a) \hat R(s,a).
          \]
          When $\hat R(s,a) = R(s,a)$, 
          \[
          \E_{\substack{s \sim p(s) \\ a \sim \pi_0(s,a)}} \qty[\E_{a \sim \pi_1(s,a)} \qty[\hat R(s,a)]] = \sum_s \sum_a \pi_1(s,a) R(s,a) p(s) = \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} [R(s,a)].
          \]
          The second term clearly vanishes when $\hat R(s,a) = R(s,a)$. Hence, the doubly robust estimator
          \[
          \E_{\substack{s \sim p(s) \\ a \sim \pi_0(s,a)}} \qty[ \E_{a \sim \pi_1(s,a)} \qty[\hat R(s,a)] + \frac{\pi_1(s,a)}{\hat \pi_0 (s,a)} (R(s,a) - \hat R(s,a)) ] = \E_{\substack{s \sim p(s) \\ a \sim \pi_1(s,a)}} [R(s,a)].
          \]
          when $\hat R(s,a) = R(s,a)$.
        \end{proof}
      \end{enumerate}

  \end{enumerate}
\end{solution}

\end{document}