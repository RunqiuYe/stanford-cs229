% HMC Math dept HW template example
% v0.04 by Eric J. Malm, 10 Mar 2005
\documentclass[12pt,letterpaper,boxed]{hmcpset}

% set 1-inch margins in the document
\usepackage[margin=1in]{geometry}
\usepackage{alltt}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathdots}
\usepackage{mathtools}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{pdflscape}
\usepackage{pgfplots}
\usepackage{siunitx}
\usepackage{slashed}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usepackage[normalem]{ulem}
\usepackage[all]{xy}
\usepackage{imakeidx}
\usepackage{enumerate}
\usepackage{physics}

% include this if you want to import graphics files with /includegraphics
\usepackage{graphicx}

\renewcommand{\tt}{t^{(i)}}
\newcommand{\ww}{w^{(i)}}
\renewcommand{\ss}{\sigma^{(i)}}
\newcommand{\ind}[1]{\mathbb{I}\{#1\}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\thetamap}{\theta_{\mathrm{MAP}}}
\renewcommand{\diag}{\mathrm{diag}}
\newcommand{\DKL}[2]{D_{\mathrm{KL}}(#1 \parallel #2)}
\newcommand{\lunsup}{\ell_{\mathrm{unsup}}}
\newcommand{\lsup}{\ell_{\mathrm{sup}}}
\newcommand{\lsemi}{\ell_{\mathrm{semi-sup}}}
\newcommand{\tht}{\theta^{(t)}}
\newcommand{\thtt}{\theta^{(t+1)}}
\newcommand{\summ}{\sum_{i=1}^m}
\newcommand{\sumtm}{\sum_{i=1}^{\tilde m}}

% info for header block in upper right hand corner
\name{Runqiu Ye}
\class{Stanford CS229}
\assignment{Problem Set \#3}
\duedate{07/29/2024}

\linespread{1.15}
\begin{document}

\problemlist{Problem Set \#3: Deep Learning \& Unsupervised Learning}

\begin{problem}[Problem 1]
  \textbf{A simple neural network}
  
  Let $X = \{ \xx[1], \xx[2], \dots, \xx[m] \}$ be dataset of $m$ examples with $2$ features. That is, $\xx \in \R^2$. Samples are classifed into $2$ categorie with labels $y \in \{0, 1\}$, as shown in Figure 1. Want to perform binary classification using a simple neural networks with the architecture shown in Figure 2.

  Two features $x_1$ and $x_2$, the three neurons in the hidden layer $h_1$, $h_2$, $h_3$, and the output neuron as $o$. Weight from $x_i$ to $h_j$ be $w_{i,j}^{[1]}$ for $i = 1,2$ and $j = 1,2,3$, and weight from $h_j$ to $o$ be $w_j^{[2]}$. Finally, denote intercept weight for $h_j$ as $w_{0,j}^{[1]}$ and the intercept weight for $o$ as $w_0^{[2]}$. Use average squared loss instead of the usual negative log-likelihood:
  \[
    l = \frac{1}{m} \sum_{i=1}^{m} (\oo - \yy) ^2.
  \]
\end{problem}

\begin{solution}
\begin{enumerate}[(a)]
  \item Suppose we use sigmoid function as activation function for $h_1$, $h_2$, $h_3$, and $o$. We have
  \[
    h_1 = g(w_1^{[1]} x), \quad h_2 = g(w_2^{[1]} x), \quad h_3 = g(w_3^{[1]} x), \quad o = g(w^{[2]} h).
  \]
  Hence, 
  \[
  \begin{aligned}
    \pdv{l}{w_{1,2}^{[1]}} &= 
      \frac{1}{m} \sum_{i=1}^{m} 2 (\oo-\yy) \oo (1-\oo) w_2^{[2]} \hh_2 (1-\hh_2) \xx_1,
  \end{aligned}
  \]
  where $\hh_2 = g(w_{0,2}^{[1]} + w_{1,2}^{[1]} \xx_1 + w_{2,2}^{[1]} \xx_2)$ and $g$ is the sigmoid function.
  Therefore, the gradient descent update to $w_{1,2}^{[1]}$, assuming learning rate $\alpha$ is
  \[
    w_{1,2}^{[1]} := w_{1,2}^{[1]} - \frac{2 \alpha}{m} \sum_{i=1}^{m} (\oo-\yy) \oo (1-\oo) w_2^{[2]} \hh_2 (1-\hh_2) \xx_1
  \]
  where $\hh_2 = g(w_{0,2}^{[1]} + w_{1,2}^{[1]} \xx_1 + w_{2,2}^{[1]} \xx_2 )$.

  \item Now, suppose the activation function for $h_1$, $h_2$, $h_3$, and $o$ is the step function $f(x)$, defined as
  \[
    f(x) = \begin{cases}
      1, & (x \geq 0), \\
      0, & (x < 0).
    \end{cases}
  \]
  Is it possible to have a set of weights that allow the neural network to classify this dataset with 100\% accuracy? If so, provide a set of weights by completing \verb|optimal_step_weights| wihin \verb|src/p01_nn.py| and explain your reasoning for those weights. If not, please explain the reasoning.

  There is a set of weights that allow the neural network to classify this dataset with 100\% accuracy. For the step function activation, we have
  \[
    \begin{aligned}
      h_1 &= f(w_{1}^{[1]} x) = f(w_{0,1}^{[1]} + w_{1,1}^{[1]} x_1 + w_{2,1}^{[1]} x_2) \\
      h_2 &= f(w_{2}^{[1]} x) = f(w_{0,2}^{[1]} + w_{1,2}^{[1]} x_1 + w_{2,3}^{[1]} x_2) \\ 
      h_3 &= f(w_{3}^{[1]} x) = f(w_{0,3}^{[1]} + w_{1,3}^{[1]} x_1 + w_{2,3}^{[1]} x_2) \\
      o &= f(w^{[2]} h) = f(w_{0}^{[2]} + w_{1}^{[2]} h_1 + w_{2}^{[2]} h_2 + w_{3}^{[2]} h_3).
    \end{aligned}
  \]
  Notice from Figure 1 that the label $\yy = 0$ if and only if $\xx$ satisfies
  \[
    \begin{cases}
      \xx_2 > 0.5, \\
      \xx_1 > 0.5, \\
      \xx_1 + \xx_2 < 4.
    \end{cases}
  \]
  Now, let
  \[
    w_1^{[1]} = \begin{bmatrix}
      0.5 \\ 0 \\ -1
    \end{bmatrix}, \quad
    w_2^{[1]} = \begin{bmatrix}
      0.5 \\ -1 \\ 0
    \end{bmatrix}, \quad
    w_3^{[1]} = \begin{bmatrix}
      -4 \\ 1 \\ 1
    \end{bmatrix}, \quad
    w_1^{[1]} = \begin{bmatrix}
      -0.5 \\ 1 \\ 1 \\ 1
    \end{bmatrix}.
  \]
  Under this set of weights, if all inequalities are satisifed, then $h_1 = h_2 = h_3 = 0$ and $w^{[2]}h = -0.5$. Otherwise, $h_1 + h_2 + h_3 \geq 1$ and $w^{[2]}h \geq -0.5$. Hence, this set of weights will capture all the conditions and allow the nerual network to classify this dataset with 100\% accuracy. 
  
  \item Let the activation function for $h_1$, $h_2$, $h_3$, be the linear function $f(x) = x$, and the activation function for $o$ be the same step function as before. Is it possible to have a set of weights that allow the neural network to classify this dataset with 100\% accuracy? If so, provide a set of weights by completing \verb|optimal_linear_weights| wihin \verb|src/p01_nn.py| and explain your reasoning for those weights. If not, please explain the reasoning.
  
  There does not exist a set of weights that allow the neural network to classify this dataset with 100\% accuracy. As the activation function for $h_1$, $h_2$, $h_3$ is the linear function, we have
  \[
    \begin{aligned}
      h_1 &= f(w_{1}^{[1]} x) = w_{0,1}^{[1]} + w_{1,1}^{[1]} x_1 + w_{2,1}^{[1]} x_2 \\
      h_2 &= f(w_{2}^{[1]} x) = w_{0,2}^{[1]} + w_{1,2}^{[1]} x_1 + w_{2,3}^{[1]} x_2 \\ 
      h_3 &= f(w_{3}^{[1]} x) = w_{0,3}^{[1]} + w_{1,3}^{[1]} x_1 + w_{2,3}^{[1]} x_2 \\
      % o &= f(w^{[2]} h) = f(w_{0}^{[2]} + w_{1}^{[2]} h_1 + w_{2}^{[2]} h_2 + w_{3}^{[2]} h_3).
    \end{aligned}
  \]
  Now, 
  \[
    \begin{aligned}
      w^{[2]} h &= w_{0}^{[2]} + w_{1}^{[2]} h_1 + w_{2}^{[2]} h_2 + w_{3}^{[2]} h_3 \\
      &= (w_{0,1}^{[1]} + w_{0,2}^{[1]} + w_{0,3}^{[1]}) + (w_{1,1}^{[1]} + w_{1,2}^{[1]} + w_{1,3}^{[1]}) x_1 + (w_{2,1}^{[1]} + w_{2,2}^{[1]} + w_{2,3}^{[1]}) x_2.
    \end{aligned}
  \]
  That is, the neural network degenerates into a linear model. However, the dataset is clearly not linearly separable. Thus, there does not exist a set of weights that allow the neural network to classify this dataset with 100\% accuracy if the activation function for $h_1$, $h_2$, $h_3$ are the linear function $f(x) = x$.
  
\end{enumerate}
\end{solution}

\begin{problem}[Problem 2]
  \textbf{KL divergence and maximum likelihood}

  Kullback-Leibler (KL) divergence is a measure of how much one probability distribution is different from a second one. The \emph{KL divergence} between two discrete-valued distribution $P(X)$, $Q(X)$ over the outcome space $\mathcal{X}$ is defined as follows:
  \[
    \DKL{P}{Q} = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}.
  \]
  Assuem $P(x) > 0$ for all $x$. (One other standard thing to do is adopt the convention that $0 \log 0 = 0$.) Sometimes, we also write the KL divergence more explicityly as $\DKL{P}{Q} = \DKL{P(X)}{Q(X)}$.

  \emph{Background on Information Theory}

  The \emph{entropy} of a probability distribution $P(X)$, defined as 
  \[
  H(P) = - \sum_{x \in \mathcal{X}} P(x) \log P(x).
  \]
  measures how dispersed a probability distribution is. Notably, $\mathcal{N}(\mu, \sigma^2)$ has the highest entropy among all possible continuous distribution that has mean $\mu$ and variance $\sigma^2$. The entropy $H(P)$ is the best possible long term average bits per message (optimal) that can be achieved under probability distribution $P(X)$.

  The \emph{cross entropy} is defined as
  \[
  H(P, Q) = - \sum_{x \in \mathcal{X}} P(x) \log Q(x).
  \]
  The cross entropy $H(P, Q)$ is the long term average bits per message (suboptimal) that results under a distribution $P(X)$, by reusing an encoding scheme designed to be optimal for a scenario with probability distribution $Q(X)$.

  Notice that 
  \[
  \DKL{P}{Q} = \sum_{x \in \mathcal{X}} P(x) \log P(x) - \sum_{x \in \mathcal{X}} P(x) \log Q(x) = H(P, Q) - H(P).
  \]
  If $H(P,Q) = 0$, then it necessarily means $P=Q$. In ML, it is common task to find distribution $Q$ that is close to another distribution $P$. To achieve this, we optimize $\DKL{P}{Q}$. Later we will see that Maximum Likelihood Estimation turns out to be equivalent minimizing KL divergence between the training data and the model.
\end{problem}

\begin{solution}
  \begin{enumerate}[(a)]
    \item \textbf{Nonnegativity.} Prove that 
    \[
    \DKL{P}{Q} \geq 0 
    \]
    and $\DKL{P}{Q} = 0$ if an only if $P = Q$.
    
    \textbf{Hint:} Use Jensen's inequality.
    
    \begin{proof}
      By definition,
      \[
        \DKL{P}{Q} = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)} = - \sum_{x \in \mathcal{X}} P(x) \log \frac{Q(x)}{P(x)}.
      \] 
      Since $-\log x$ is strictly convex, by Jensen's inequality, we have
      \[
      \begin{aligned}
        \DKL{P}{Q} = - \sum_{x \in \mathcal{X}} P(x) \log \frac{Q(x)}{P(x)}
        \geq -\log \sum_{x \in \mathcal{X}} P(x) \frac{Q(x)}{P(x)} = 0.
      \end{aligned}
      \]
      When the equality holds, 
      \[
      \log \frac{Q(x)}{P(x)} = 0
      \]
      with probability 1. That is, $Q = P$ with probability 1. This completes the proof.
    \end{proof}
    
    \item \textbf{Chain rule for KL divergence.} The KL divergence between 2 conditional distributions $P(X \mid Y)$, $Q(X \mid Y)$ is defined as follows:
    \[
    \DKL{P(X \mid Y)}{Q(X \mid Y)} = \sum_y P(y) \qty(\sum_x P(x \mid y) \log \frac{P(x \mid y)}{Q(x \mid y)}).
    \]
    This can be thought of as the expected KL divergence between the corresponding conditional distributions on $x$. That is, between $P(X \mid Y = y)$ and $Q(X \mid Y = y)$, where the expectation is taken over the random y.

    Prove the following chain rule for KL divergence:
    \[
      \DKL{P(X, Y)}{Q(X, Y)} = \DKL{P(X)}{Q(X)} + \DKL{P(Y \mid X)}{Q(Y \mid X)}.
    \]
    \begin{proof}
      \[
      \begin{aligned}
        \text{LHS} &= \sum_x \sum_y P(x,y) \log \frac{P(x,y)}{Q(x,y)} \\
        &= \sum_x \sum_y P(y \mid x) P(x) \qty[\log \frac{P(y \mid x)}{Q(y \mid x)} + \log \frac{P(x)}{Q(x)}] \\
        &= \sum_x \sum_y P(y \mid x) P(x) \log \frac{P(y \mid x)}{Q(y \mid x)} + \sum_x P(x) \log \frac{P(x)}{Q(x)} \sum_y P(y \mid x) \\
        &= \sum_x \sum_y P(y \mid x) P(x) \log \frac{P(y \mid x)}{Q(y \mid x)} + \sum_x P(x) \log \frac{P(x)}{Q(x)} \\
        &= \DKL{P(X)}{Q(X)} + \DKL{P(Y \mid X)}{Q(Y \mid X)} \\
        &= \text{RHS}.
      \end{aligned}
      \]
    \end{proof}

    \item \textbf{KL and maximum likelihood.} Consider density estimation problem and suppose we are given training set $\{\xx\}_{i=1}^m$. Let the empirical distribution be $\hat P(x) = \frac{1}{m} \sum_{i=1}^{m} \ind{\xx = x}$. ($\hat P$ is just the uniform distribution over the training set; i.e., sampling from the empirical distribution is the same as picking a random example from the training set.)
    
    Suppose we have a family of distributions $P_\theta$ parametrized by $\theta$. Prove that finding the maximum likelihood estimates for the parameter $\theta$ is equivalent to finding $P_\theta$ with minimal KL divergence from $\hat P$. That is, prove that
    \[
    \argmin_\theta \DKL{\hat P}{P_\theta} = \argmax_\theta \sum_{i=1}^m \log P_\theta (\xx).
    \]

    \begin{proof}
      Notice that $\hat P$ is the uniform distribution over the training set, thus $\hat P(\xx) = \frac{1}{m}$ for $i = 1, \dots, m$. It follows that
      \[
      \DKL{\hat P}{P_\theta} = \sum_x \hat P(x) \log \frac{\hat P(x)}{P_\theta(x)} = - \log m - \frac{1}{m} \sum_{i=1}^m \log P_\theta(\xx).
      \]
      Hence,
      \[
      \argmin_\theta \DKL{\hat P}{P_\theta} = \argmax_\theta \sum_{i=1}^m \log P_\theta (\xx),
      \]
      as desired.
    \end{proof}
  \end{enumerate}
\end{solution}

\begin{remark}
  \textbf{Remark:} Consider the relationship between parts (b-c) and multi-variate Bernoulli Naive bayes parameter estimation. In Naive Bayes model we assumed $P_\theta$ is the following form: $P_\theta(x,y) = p(y) \prod_{i=1}^n p(x_i \mid y)$. By the chain rule for KL divergence, we therefore have  
  \[
  \DKL{\hat P}{P_\theta} = \DKL{\hat P(y)}{p(y)} + \sum_{i=1}^n \DKL{\hat P(x_i \mid y)}{p(x_i \mid y)}.
  \]
  This shows that finding the maximum likelihood/minimum KL divergence estimates of the parameters decomposes into $2n+1$ independent optimization problems: One for the class priors $p(y)$, and one for each conditional distributions $p(x_i \mid y)$ for each feature $x_i$ given each of the two possible labels for $y$. Specifically, finding the maximum likelihood estimates for each of these problems individually results in also maximizing the likelihood of the joint distribution. This similarly applies to bayesian networks.
\end{remark}

\begin{problem}[Problem 3]
  \textbf{KL divergence, Fisher Information, and the Natural Gradient}

  KL divergence between the two distributions is an asymetric measure of how different two distributions are. Consider two distributions over the same space given by densities $p(x)$, $q(x)$. The KL divergence between two continuous distributions is defined as
  \[
  \begin{aligned}
    \DKL{p}{q} &= \int p(x) \log \frac{p(x)}{q(x)} dx \\
    &= \E_{x \sim p(x)}[\log p(x)] - \E_{x \sim p(x)} [\log q(x)].
  \end{aligned}
  \]
  A nice property of KL divergence is that it is invariant to parametrization. This means, KL divergence evaluates to the same value no matter how we parametrize the distribution $P$ and $Q$. For example, if $P$ and $Q$ are in exponential family, the KL divergence between them is the same whether we are using natural parameters, natural parameters, or canonical parameters, or any arbitrary parametrization.

  Now consider the problem of fitting model parameters using gradient descent. While KL divergence is invariant to parametrization, the gradient w.r.t the model parameters gradient is \emph{invariant to parametrization}. We need to use \emph{natural gradient}. This will make the optimization process invariant to the parametrization.

  We will construct and derive the natural gradient update rule. Along the way, we will introduce \emph{score function} and \emph{Fisher Information}. Finally, we will see how this new natural gradient based optimization is actually equivalent to Newton's method for Generalized Linear Models.

  Let the distribution of a random variable $Y$ parametrized by $\theta \in \R^n$ be $p(y; \theta)$.
\end{problem}

\begin{solution}
\begin{enumerate}[(a)]
  \item \textbf{Score function.} The \emph{score function} with $p(y; \theta)$ is defined as $\nabla_\theta \log p(y; \theta)$, which signifies the sensitivity of the likelihood function with respect to the parameters.
  
  Show that the expected value of the score is 0.

  \begin{proof}
    The expected value of the score
    \[
    \begin{aligned}
      \E_{y \sim p(y; \theta)}[[\nabla_{\theta'} \log p(y; \theta)]_{\theta' = \theta}] &= \int p(y; \theta) \qty[\nabla_{\theta'} \log p(y; \theta)]_{\theta' = \theta} dy \\
      &= \int p(y; \theta) \frac{1}{p(y; \theta) } \qty[\nabla_{\theta'} p(y; \theta)]_{\theta' = \theta}dy \\
      &= \qty[\nabla_{\theta'} \int p(y; \theta) dy]_{\theta' = \theta}\\
      &= 0
    \end{aligned}
    \]
  \end{proof}

  \item \textbf{Fisher information.} \emph{Fisher information} is defined as the covariance matrix of the score function,
  \[
  \mathcal{I}(\theta) = \cov_{y \sim p(y; \theta)} \qty[\nabla_{\theta'} \log p(y; \theta') ]_{\theta' = \theta}.
  \]
  Intuitively, the Fisher information represents the amount of information that a random variable $Y$ carries about a parameter $\theta$ of interest. Show that the Fisher information can be equivalently given by
  \[
  \mathcal{I}(\theta) = \E_{y \sim p(y; \theta)} \qty[ \qty[\nabla_{\theta'} \log p(y; \theta') \nabla_{\theta'} \log p(y; \theta')^T ]_{\theta' = \theta}].
  \]

  Note that the fisher information is a funciton of the parameter. The parameter is both a) the parameter value at which the score function is evaluated, and b) the parameter of the distribution with respect to which the expectation and variance is calculated.

  \begin{proof}
    Since $\E_{y \sim p(y; \theta)}[[\nabla_{\theta'} \log p(y; \theta)]_{\theta' = \theta}] = 0$, we have
    \[
      \cov_{y \sim p(y; \theta)} \qty[\nabla_{\theta'} \log p(y; \theta') ]_{\theta' = \theta} = \E_{y \sim p(y; \theta)} \qty[ \qty[\nabla_{\theta'} \log p(y; \theta') \nabla_{\theta'} \log p(y; \theta')^T ]_{\theta' = \theta}]
    \]
    by the definition of covariance. This completes the proof.
  \end{proof}

  \item \textbf{Fisher information (alternate form).} It turns out that Fisher information can not only be defined as the covariance of the score functino, but in most situations it can also be represented as the expected negative Hessian of the log-likelihood. Show that
  \[
  \E_{y \sim p(y; \theta)} \qty[\qty[ - \nabla^2_{\theta'} \log p(y; \theta')]_{\theta' = \theta}] = \mathcal{I}(\theta).
  \]
  \begin{proof}
    From (b), we know that 
    \[
    \begin{aligned}
      \mathcal{I}_{ij}(\theta) &= \E \qty[\pderivative{\theta_i} \log p(y; \theta') \pderivative{\theta_j} \log p(y; \theta')] \\
      &= \E \qty[\frac{1}{p(y; \theta')^2} \pderivative{\theta_i}p(y; \theta') \pderivative{\theta_j} p(y; \theta')].
    \end{aligned}
    \]
    Also, for the left hand side of the expression we need to prove, we have
    \[
    \begin{aligned}
      \text{LHS} &= \E \qty[-\pderivative{\theta_i}\pderivative{\theta_j} \nabla^2_{\theta'} p(y; \theta')] \\
      &= \E \qty[\pderivative{\theta_i} \frac{1}{p(y; \theta')} \pderivative{\theta_i} p(y; \theta')] \\
      &= \E \qty[ -\frac{1}{p(y; \theta')}  \pderivative{\theta_i}p(y; \theta') \pderivative{\theta_j} p(y; \theta') + \frac{1}{p(y; \theta')} \pderivative{\theta_i} \pderivative{\theta_j}p(y; \theta')]
    \end{aligned}
    \]
    For the second term, we have
    \[
    \begin{aligned}
      \E \qty[\frac{1}{p(y; \theta')} \pderivative{\theta_i} \pderivative{\theta_j}p(y; \theta')] &= \int \pderivative{\theta_i} \pderivative{\theta_j} p(y; \theta') dy \\
      &= \pderivative{\theta_i} \pderivative{\theta_j} \int p(y; \theta') dy \\
      &= 0.
    \end{aligned}
    \]
    Hence, 
    \[
      \E_{y \sim p(y; \theta)} \qty[\qty[ - \nabla^2_{\theta'} \log p(y; \theta')]_{\theta' = \theta}] = \E \qty[ -\frac{1}{p(y; \theta')}  \pderivative{\theta_i}p(y; \theta') \pderivative{\theta_j} p(y; \theta')] = \mathcal{I}(\theta).
    \]
    This completes the proof.
  \end{proof}
  \begin{remark}
    \textbf{Remark.} This shows that the expected curvature of the log-likelihood function is also equal to the Fisher information matrix. If the curvature of the log-likelihood is steep, this generally means you need fewer number of data samples to estimate that parameter well, and vice versa. The fisher information matrix associated with a statistical model parameterized by $\theta$ is extremely important in determining how a model behaves as a function of the number of traning set examples.
  \end{remark}

  \item \textbf{Approximatin $D_{\mathrm{KL}}$ with Fisher information.} We are interested in the set of all distributions that are at a small fixed $D_{\mathrm{KL}}$ distance away from the current distribution. To calculate KL divergence between $p(y; \theta)$ and $p(y; \theta + d)$, we approximate with Fisher information at $\theta$. Show that
  \[
  \DKL{p_\theta}{p_{\theta + d}} \approx \frac{1}{2} d^T \mathcal{I}(\theta) d.
  \]
  \begin{proof}
    Towards Taylor expansion, we have
    \[
    \begin{aligned}
      & \DKL{p_\theta}{p_\theta} = 0 \\
      & \nabla_{\theta'} \DKL{p_\theta}{p_{\theta'}} = \nabla_{\theta'} \E[\log p_\theta] - \E[\log p_{\theta'}] = 0 \\ 
      &\nabla_{\theta'}^2 \DKL{p_\theta}{p_{\theta'}} = \nabla_{\theta'}^2 \E[\log p_\theta] - \E[\log p_{\theta'}] = \mathcal{I}(\theta).
    \end{aligned}
    \]
    Hence,
    \[
    \DKL{p_\theta}{p_{\tilde{\theta}}} \approx \frac{1}{2} d^T \mathcal{I}(\theta) d.
    \]
  \end{proof}

  \item \textbf{Natural gradient.} Want to maximize the log-likelihood by moving only by a fixed $D_{\mathrm{KL}}$ distance from the current position. Now set up the constrained optimization problem that will yield the natural gradient update $d$. Let the log-likelihood objective be $\ell(\theta) = \log p(y; \theta)$. Let the $D_{\mathrm{KL}}$ distance we want to move by be some small positive constant $c$. The natural gradient update $d^*$ is
  \[
  d^* = \argmax_d \ell(\theta + d) \text{ subject to } \DKL{p_\theta}{p_{\theta+d}} = c.
  \]
  In order to solve this, use Taylor expansion and Lagrangian multipliers.

  For the optimization problem, consider the Lagrangian
  \[
  \begin{aligned}
    L(d, \lambda) &= \ell(\theta + d) - \lambda \qty(\DKL{p_\theta}{p_{\theta+d}} - c) \\
    &= \log p(y; \theta + d) - \lambda \qty(\DKL{p_\theta}{p_{\theta+d}} - c) \\
    &= \log p(y; \theta) + d^T \frac{\nabla_{\theta} p(y;\theta)}{p(y; \theta)} - \lambda \qty(\frac{1}{2} d^T \mathcal{I}(\theta) d - c).
  \end{aligned}
  \]
  Set 
  \[
  \begin{cases}
    \nabla_d L(d, \lambda) = 0, \\
    \nabla_\lambda L(d, \lambda) = 0. 
  \end{cases}
  \implies
  \begin{cases}
    \frac{\nabla_{\theta} p(y;\theta)}{p(y; \theta)} = \lambda \mathcal{I}(\theta) d, \\
    \frac{1}{2} d^T \mathcal{I}(\theta) d = c.
  \end{cases}
  \]
  This gives
  \[
  d = \sqrt{\frac{2c}{\nabla_{\theta} p(y;\theta) ^T (\mathcal{I}^{-1})^T \nabla_{\theta} p(y;\theta)}} \mathcal{I}^{-1} \nabla_{\theta} p(y;\theta).
  \]
  
  \item \textbf{Relation to Newton's Method.} Show that the direction of update of Newton's method, and the direction of natural gradient, are exactly the same for GLMs. Refer to results in problem set 1 question 4. For natural gradient, it is sufficient to use $\tilde{d}$, the unscaled natural gradient.
  
  \begin{proof}
    Recall that for GLMs, we have
    \[
    p(y; \eta) = b(y) \exp(\eta^T T(y) - a(\eta)).
    \]
    It follows that
    \[
    \begin{aligned}
      \log p(y; \eta) &= \log b(y) + \eta^T T(y) - a(\eta), \\
      \nabla_\eta \log p(y; \eta) &= T(y) - \nabla_\eta a(\eta), \\
      \nabla_\eta^2 \log p(y; \eta) &= - \nabla_\eta^2 a(\eta).
    \end{aligned}
    \]
    When using Newton's method to update, we have
    \[
    \begin{aligned}
      \eta &:= \eta - \qty(\nabla_\eta^2 \log p(y; \eta))^{-1} \nabla_\eta \log p(y; \eta) \\
      &= \eta + \frac{1}{p(y; \eta)} \qty(\nabla_\eta^2 a(\eta))^{-1}\nabla_\eta p(y; \eta).
    \end{aligned}
    \]
    Compare this the natural gradient
    \[
    d = \frac{1}{\lambda p(y;\eta)} \mathcal{I}^{-1} \nabla_\eta p(y; \eta).
    \]
    Recall that Fisher information matrix is the negative expected value of Hessian, and notice that the Hessian here is independent of $y$. Hence,
    \[
    \mathcal{I} = \E_{y \sim p(y; \theta)} \qty[-\nabla^2_\eta \log p(y; \eta)] = \E_{y \sim p(y; \theta)} \qty[\nabla_\eta^2 a(\eta)] = \nabla_\eta^2 a(\eta).
    \]
    From this, we can clearly see that the direction of update of Newton's method and the direction of natural gradient are exactly the same for GLMs. This completes the proof.
  \end{proof}
\end{enumerate}
\end{solution}

\begin{problem}[Problem 4]
  \textbf{Semi-supervised EM}

  Expectation maximization (EM) is a classical algorithm for unsupervised learning. In this problem we explore one ways in which EM can be adapted to semi-supervised setting, where we have some labelled examples along with unlabelled examples.

  In unsupervised learning, we have $\{\xx\}_{i=1}^m$ unlabelled examples and we wish to learn $p(x, z; \theta)$, but $\zz$ are not observed. EM allow us to maximize the intractable $p(x; \theta)$ indirectly by iteratively performing the E-step and M-step, each time maximizing a tractable lower bound. The objective
  \[
  \begin{aligned}
    \ell_{\mathrm{unsup}}(\theta) = \sum_{i=1}^m \log p(\xx; \theta) = \sum_{i=1}^m \log \sum_{\zz} p(\xx, \zz; \theta).
  \end{aligned}
  \]
  Now, extend EM to semi-supervised setting. Suppose \emph{additional} $\tilde{m}$ labelled examples $\{(\txx, \tzz)\}_{i=1}^{\tilde{m}}$ where $x$ and $z$ are both observed. Want to simultaneously maximize the marginal likelihood of the parameters using the unlabelled examples, and full likelihood of the parameters using the labelled examples, by optimizing their weighted sum with some hyperparameter $\alpha$. More concretely, the semi-supervised objective $\lsemi$ can be written as 
  \[
  \begin{aligned}
    \lsup(\theta) &= \sum_{i=1}^{\tilde{m}} \log p(\txx, \tzz; \theta), \\
    \lsemi(\theta) &= \lunsup + \alpha \lsup.
  \end{aligned}
  \]
\end{problem}

\begin{solution}
  First, we derive the EM steps for semi-supervised learning.
  \begin{itemize}
    \item \textbf{E-step (semi-supervised)}
    
    For $i = 1, 2, \dots, m$, set
    \[
    Q_i^{(t)} (\zz) = p(\zz \mid \xx; \theta^{(t)}).
    \]

    \item \textbf{M-step (semi-supervised)}
    \[
    \theta^{(t+1)} = \argmax_{\theta} \qty[ \sum_{i=1}^m \qty(\sum_{\zz} Q_i(\zz) \log \frac{p(\xx, \zz; \theta)}{Q_i(\zz)} )+ \alpha \qty(\sum_{i=1}^{\tilde{m}} \log p(\txx, \tzz; \theta))].
    \]
  \end{itemize}

  \begin{enumerate}[(a)]
    \item \textbf{Convergence.} First show that this algorithm converges. It suffices to show that the objective function $\lsemi(\theta)$ with each iteration of E-step and M-step. That is, we need to show that $\lsemi(\theta^{(t+1)}) \geq \lsemi(\theta^{(t)})$  
    
    \begin{proof}
      For $\tht$ and the $Q_i^{(t)}$ chosen, it is guaranteed that
      \[
      \begin{aligned}
        \lsemi(\tht) &= \sum_{i=1}^m \log \sum_{\zz} p(\xx, \zz;\tht) + \alpha \sum_{i=1}^{\tilde{m}} \log p(\txx, \tzz; \tht) \\
        &= \sum_{i=1}^m \sum_{\zz} Q_i^{(t)}(\zz) \log \frac{p(\xx, \zz;\tht)}{Q_i^{(t)}(\zz)} + \alpha \sum_{i=1}^{\tilde{m}} \log p(\txx, \tzz; \tht).
      \end{aligned}
      \]
      Since $Q_i^{(t)}$ is a probability measure, for $\thtt$, we also have
      \[
      \begin{aligned}
        \lsemi(\thtt) &= \sum_{i=1}^m \log \sum_{\zz} p(\xx, \zz;\thtt) + \alpha \sum_{i=1}^{\tilde{m}} \log p(\txx, \tzz; \thtt) \\
        &\geq \sum_{i=1}^m \sum_{\zz} Q_i^{(t)}(\zz) \log \frac{p(\xx, \zz;\thtt)}{Q_i^{(t)}(\zz)} + \alpha \sum_{i=1}^{\tilde{m}} \log p(\txx, \tzz; \thtt)
      \end{aligned}
      \]
      by Jensen's inequality.
      
      As $\thtt$ maximizes the expression in M-step, we have
      \[
      \begin{aligned}
        \lsemi(\thtt) 
        &\geq \sum_{i=1}^m \sum_{\zz} Q_i^{(t)}(\zz) \log \frac{p(\xx, \zz;\thtt)}{Q_i^{(t)}(\zz)} + \alpha \sum_{i=1}^{\tilde{m}} \log p(\txx, \tzz; \thtt) \\
        &\geq \sum_{i=1}^m \sum_{\zz} Q_i^{(t)}(\zz) \log \frac{p(\xx, \zz;\tht)}{Q_i^{(t)}(\zz)} + \alpha \sum_{i=1}^{\tilde{m}} \log p(\txx, \tzz; \tht) \\
        &= \lsemi(\tht).
      \end{aligned}
      \]
      This completes the proof.
    \end{proof}
  \end{enumerate}

  \noindent \textbf{Semi-supervised GMM}

  \noindent Now we revisit the Gaussian Mixture Model (GMM) to apply our semi-supervised EM algorithm. Consider a schenario where data is generated from $k$ Gaussian distributions with unknown means $\mu_j \in \R^d$ and covariances $\Sigma_j \in \mathbb{S}_+^d$ where $j = 1, \dots, k$. We have $m$ data points $\xx \in \R^d$ and each has a corresponding latent $\zz \in \{1, \dots, k\}$ indicating which distribution $\xx$ belongs to. Specifically, $\zz \sim \text{Multinomial}(\phi)$ and $\xx \mid \zz \sim \mathcal{N}(\mu_{\zz}, \Sigma_{\zz})$ i.i.d. We also have additional $\tilde{m}$ points $\txx \in \R^d$ and an associated \emph{observed} variable $\tzz \in \{1, \dots, k\}$. As before, we assume $\txx \mid \tzz \sim \mathcal{N}(\mu_{\tzz}, \Sigma_{\tzz})$ i.i.d.

  \begin{enumerate}[(a)]
    \setcounter{enumi}{1}
    \item \textbf{Semi-supervised E-step.} In E-step, $\zz$ need to be re-estimated. For this specific model, we have
    \[
    \begin{aligned}
      \ww_j &= p(\zz = j \mid \xx) \\
      &= \frac{p(\xx \mid \zz = j) p(\zz = j)}{\sum_l p(\xx \mid \zz = l)p(\zz = l)} \\
      &= \frac{\frac{1}{\abs{\Sigma_j}^{1/2}} \exp(-\frac{1}{2} (\xx - \mu_j)^T \Sigma_j^{-1} (\xx - \mu_j)) \phi_j}{\sum_l \frac{1}{\abs{\Sigma_l}^{1/2}} \exp(-\frac{1}{2} (\xx - \mu_l)^T \Sigma_l^{-1} (\xx - \mu_l)) \phi_l}.
    \end{aligned}
    \]

    \item \textbf{Semi-supervised M-step.} The model parameters $\mu$, $\Sigma$, $\phi$ need to be re-estimated in the M-step. For this specific model, we have the optimization problem
    \[
    \argmax_{\mu, \Sigma, \phi} \lsemi = \argmax_{\mu, \Sigma, \phi} \lunsup + \alpha \lsup, 
    \]
    where
    \[
    \begin{aligned}
      \lunsup &= \sum_{i=1}^m \sum_{\zz} \ww_j \log \frac{\exp(-\frac{1}{2} (\xx - \mu_j)^T \Sigma_j^{-1} (\xx - \mu_j)) \phi_j}{(2\pi)^{d/2} \abs{\Sigma_j}^{1/2} \ww_j}, \\
      \lsup &= \sum_{i=1}^{\tilde{m}} \log \frac{1}{(2\pi)^{d/2} \abs{\Sigma_{\tzz}}^{1/2}} \exp(-\frac{1}{2} (\txx - \mu_{\tzz})^T \Sigma_{\tzz}^{-1} (\txx - \mu_{tzz})) \phi_{\tzz}.
    \end{aligned}
    \]
    To get a closed form expression, we use Lagrangian multipliers and construct Lagrangian
    \[
    L(\mu, \Sigma, \phi, \lambda) = \lunsup + \alpha \lsup + \lambda (\Sigma_j \phi_j - 1).
    \]
    For matrix derivatvies, we have
    \[
    \begin{aligned}
      &\nabla_A \abs{A} = \frac{1}{\abs{A}} A^{-T}, \\
      &\nabla_A x^T A^{-1} x = - A^{-1} x x^T A^{-1}.
    \end{aligned}
    \]
    Notice that the covariance matrices $\Sigma_l$ are symmetric, so $\Sigma_l^{-T} = \Sigma^{-1}$. It follows that
    \[
    \begin{aligned}
      \nabla_{\mu_l} \lsemi &= \sum_{i=1}^{m} \ww_l \Sigma_l^{-1} (\xx - \mu_l) + \alpha \sum_{i=1}^{\tilde{m}} \ind{\tzz = l} \Sigma_l^{-1} (\txx - \mu_l), \\
      \nabla_{\phi_l} \lsemi &=  \sum_{i=1}^m \frac{\ww_l}{\phi_l} + \alpha \sum_{i=1}^{\tilde{m}} \ind{\tzz = l} \frac{1}{\phi_l} + \lambda, \\
      \nabla_{\Sigma_l} \lsemi &= \sum_{i=1}^m -\frac{\ww_l}{2}\Sigma_l^{-1} + \frac{\ww_l}{2} \Sigma_l^{-1} (\xx - \mu_l)(\xx - \mu_l)^T \Sigma_l^{-1} \\
      & \qquad + \alpha \sum_{i=1}^{\tilde{m}} - \frac{\ind{\tzz = l} }{2}\Sigma_l^{-1} + \frac{\ind{\tzz = l } }{2} \Sigma_l^{-1} (\txx - \mu_l)(\txx - \mu_l)^T \Sigma_l^{-1}.
    \end{aligned}
    \]
    Setting the gradients to zero, we get
    \[
    \begin{aligned}
      \mu_l &= \frac{\sum_{i=1}^m \ww_l \xx + \alpha \sum_{i=1}^{\tilde{m}} \ind{\tzz = l} \txx}{\sum_{i=1}^m \ww_l + \alpha \sum_{i=1}^{\tilde{m}} \ind{\tzz = l}}, \\
      \phi_l &= - \frac{1}{\lambda} \qty(\summ \ww_l + \alpha \sumtm \ind{\tzz = l}), \\
      \Sigma_l &= \frac{\summ \ww_l (\xx - \mu_l)(\xx - \mu_l)^T + \alpha \sumtm \ind{\tzz = l} (\xx - \mu_l)(\xx - \mu_l)^T}{\summ \ww_l + \alpha \sumtm \ind{\tzz = l}}.
    \end{aligned}
    \]
    As $\sum_j \phi_j = 1$, we have
    \[
    \lambda = - \summ \sum_j \ww_j - \alpha \sumtm \sum_j\ind{\tzz = j} = - m - \alpha \tilde{m}.
    \]
    Hence,
    \[
    \phi_l = \frac{\summ \ww_l + \alpha \sumtm \ind{\tzz = l}}{m + \alpha \tilde{m}}.
    \]
    These give the closed form update rules for the model parameters based on the semi-supervised objective.

    \item \textbf{Coding problem. Classical (Unsupervised) EM Implementation.} After updating the model parameters, we can calculate the log-likelihood
    \[
    \begin{aligned}
      \lsemi &= \summ \log \sum_{\zz} p(\xx, \zz) + \alpha \sumtm \log p(\txx, \tzz) \\
      &= \summ \log \sum_{\zz} p(\xx \mid \zz) p(\zz) + \alpha \sumtm \log p(\txx, \tzz) \\
      &= \summ \log \sum_{\zz} \frac{\phi_{\zz}}{(2\pi)^{d/2} \abs{\Sigma_{\zz}}^{1/2}} \exp(-\frac{1}{2}(\xx - \mu_{\zz})^T \Sigma_{\zz} (\xx - \mu_{\zz})) \\
      & \qquad + \alpha \sumtm \log \frac{1}{(2\pi)^{d/2} \abs{\Sigma_{\tzz}}^{1/2}} \exp(-\frac{1}{2}(\txx - \mu_{\tzz})^T \Sigma_{\tzz} (\txx - \mu_{\tzz}))
    \end{aligned}
    \]

    \item \textbf{Coding problem. Semi-supervised EM Implementation.} Now consider both labelled and unlabelled examples (a total of $m + \tilde{m}$) . with 5 labelled examples per cluster.
    
    \item \textbf{Comparison of unsupervised and semi-supervised EM.} Compared to classical unsupervised EM, semi-supervised EM usually takes fewer iterations to converge, has more stability against random initilaizations, and has better overall quality.
  \end{enumerate}
\end{solution}

\begin{problem}[Problem 5]
  \textbf{K-means for compression}

  Use k-means algorithm to lossy image compression, by reducing the number of colors used in an image.

  A 512x512 image 24-bit colors takes about $512 \times 512 \times 3 = 786432 \text{ bytes }$. To compress, we use k-means to reduce the image to $k = 16$ colors. More specifically, each pixel in the image is considered a point in the 3-dimensional $(r,g,b)$ space. To compress, we will cluster these points in colo-space into 16 clusters and replace each pixel with the closest cluster centroid.
\end{problem}

\begin{solution}
  \begin{enumerate}[(a)]
    \item \textbf{Coding problem. K-means compression Implementation.} See \verb|data/p05_kmeans|.
    
    \item \textbf{Compression factor.} If we represent the image with these reduced (16) colors, we only need to store the 24-bit RGB value for the 16 centroids and the index of centroids for every pixel. We can use a 16-bit integer for the latter, and that would be
    \[
    16 \times 3 + 512 \times 512 \times 2 = 524336 \text{ bytes}
    \]
    in total. The compression factor is about $0.66 \approx \frac{2}{3}$, since we only need 2 bytes instead of 3 for each pixel.
  \end{enumerate}
\end{solution}

\end{document}
