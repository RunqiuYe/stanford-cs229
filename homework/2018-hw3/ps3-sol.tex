% HMC Math dept HW template example
% v0.04 by Eric J. Malm, 10 Mar 2005
\documentclass[12pt,letterpaper,boxed]{hmcpset}

% set 1-inch margins in the document
\usepackage[margin=1in]{geometry}
\usepackage{alltt}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{mathdots}
\usepackage{mathtools}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{pdflscape}
\usepackage{pgfplots}
\usepackage{siunitx}
\usepackage{slashed}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usepackage[normalem]{ulem}
\usepackage[all]{xy}
\usepackage{imakeidx}
\usepackage{enumerate}
\usepackage{physics}

% include this if you want to import graphics files with /includegraphics
\usepackage{graphicx}

\renewcommand{\tt}{t^{(i)}}
\newcommand{\ww}{w^{(i)}}
\renewcommand{\ss}{\sigma^{(i)}}
\newcommand{\ind}[1]{\mathbb{I}\{#1\}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\thetamap}{\theta_{\mathrm{MAP}}}
\renewcommand{\diag}{\mathrm{diag}}


% info for header block in upper right hand corner
\name{Runqiu Ye}
\class{Stanford CS299}
\assignment{Problem Set \#3}
\duedate{07/29/2024}

\linespread{1.15}
\begin{document}

\problemlist{Problem Set \#3: Deep Learning \& Unsupervised Learning}

\begin{problem}[Problem 1]
  \textbf{A simple neural network}
  
  Let $X = \{ \xx[1], \xx[2], \dots, \xx[m] \}$ be dataset of $m$ examples with $2$ features. That is, $\xx \in \R^2$. Samples are classifed into $2$ categorie with labels $y \in \{0, 1\}$, as shown in Figure 1. Want to perform binary classification using a simple neural networks with the architecture shown in Figure 2.

  Two features $x_1$ and $x_2$, the three neurons in the hidden layer $h_1$, $h_2$, $h_3$, and the output neuron as $o$. Weight from $x_i$ to $h_j$ be $w_{i,j}^{[1]}$ for $i = 1,2$ and $j = 1,2,3$, and weight from $h_j$ to $o$ be $w_j^{[2]}$. Finally, denote intercept weight for $h_j$ as $w_{0,j}^{[1]}$ and the intercept weight for $o$ as $w_0^{[2]}$. Use average squared loss instead of the usual negative log-likelihood:
  \[
    l = \frac{1}{m} \sum_{i=1}^{m} (\oo - \yy) ^2.
  \]
\end{problem}

\begin{solution}
\begin{enumerate}[(a)]
  \item Suppose we use sigmoid function as activation function for $h_1$, $h_2$, $h_3$, and $o$. We have
  \[
    h_1 = g(w_1^{[1]} x), \quad h_2 = g(w_2^{[1]} x), \quad h_3 = g(w_3^{[1]} x), \quad o = g(w^{[2]} h).
  \]
  Hence, 
  \[
  \begin{aligned}
    \pdv{l}{w_{1,2}^{[1]}} &= 
      \frac{1}{m} \sum_{i=1}^{m} 2 (\oo-\yy) \oo (1-\oo) w_2^{[2]} \hh_2 (1-\hh_2) \xx_1,
  \end{aligned}
  \]
  where $\hh_2 = g(w_{0,2}^{[1]} + w_{1,2}^{[1]} \xx_1 + w_{2,2}^{[1]} \xx_2)$ and $g$ is the sigmoid function.
  Therefore, the gradient descent update to $w_{1,2}^{[1]}$, assuming learning rate $\alpha$ is
  \[
    w_{1,2}^{[1]} := w_{1,2}^{[1]} - \frac{2 \alpha}{m} \sum_{i=1}^{m} (\oo-\yy) \oo (1-\oo) w_2^{[2]} \hh_2 (1-\hh_2) \xx_1
  \]
  where $\hh_2 = g(w_{0,2}^{[1]} + w_{1,2}^{[1]} \xx_1 + w_{2,2}^{[1]} \xx_2 )$.

  \item Now, suppose the activation function for $h_1$, $h_2$, $h_3$, and $o$ is the step function $f(x)$, defined as
  \[
    f(x) = \begin{cases}
      1, & (x \geq 0), \\
      0, & (x < 0).
    \end{cases}
  \]
  Is it possible to have a set of weights that allow the neural network to classify this dataset with 100\% accuracy? If so, provide a set of weights by completing \verb|optimal_step_weights| wihin \verb|src/p01_nn.py| and explain your reasoning for those weights. If not, please explain the reasoning.

  There is a set of weights that allow the neural network to classify this dataset with 100\% accuracy. For the step function activation, we have
  \[
    \begin{aligned}
      h_1 &= f(w_{1}^{[1]} x) = f(w_{0,1}^{[1]} + w_{1,1}^{[1]} x_1 + w_{2,1}^{[1]} x_2) \\
      h_2 &= f(w_{2}^{[1]} x) = f(w_{0,2}^{[1]} + w_{1,2}^{[1]} x_1 + w_{2,3}^{[1]} x_2) \\ 
      h_3 &= f(w_{3}^{[1]} x) = f(w_{0,3}^{[1]} + w_{1,3}^{[1]} x_1 + w_{2,3}^{[1]} x_2) \\
      o &= f(w^{[2]} h) = f(w_{0}^{[2]} + w_{1}^{[2]} h_1 + w_{2}^{[2]} h_2 + w_{3}^{[2]} h_3).
    \end{aligned}
  \]
  Notice from Figure 1 that the label $\yy = 0$ if and only if $\xx$ satisfies
  \[
    \begin{cases}
      \xx_2 > 0.5, \\
      \xx_1 > 0.5, \\
      \xx_1 + \xx_2 < 4.
    \end{cases}
  \]
  Now, let
  \[
    w_1^{[1]} = \begin{bmatrix}
      0.5 \\ 0 \\ -1
    \end{bmatrix}, \quad
    w_2^{[1]} = \begin{bmatrix}
      0.5 \\ -1 \\ 0
    \end{bmatrix}, \quad
    w_3^{[1]} = \begin{bmatrix}
      -4 \\ 1 \\ 1
    \end{bmatrix}, \quad
    w_1^{[1]} = \begin{bmatrix}
      -0.5 \\ 1 \\ 1 \\ 1
    \end{bmatrix}.
  \]
  This set of weights will capture all the conditions and allow the nerual network to classify this dataset with 100\% accuracy.

\end{enumerate}
\end{solution}

\end{document}